[
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker altered data or model to modify the ML algorithm's behavior in a chosen direction (e.g. to sabotage its results, to insert a backdoor). It is as if the attacker conditioned the algorithm according to its motivations.\nSuch attacks are also called causative attacks.\nExample: massively indicating to an image recognition algorithm that images of dogs are indeed cats to lead it to interpret it this way.",
    "Vulnerability (consequence)": "Implement processes to maintain security levels of ML components over time",
    "Action": "(Specific ML) Choose and define a more resilient model design: \nSome model designs can be more robust than others against attacks. For instance, ensemble methods like bagging can mitigate the impact of poisoning during the training phase (Bagging for fighting <a href='https://link.springer.com/chapter/10.1007/978-3-642-21557-5_37'>poisoning attacks </a>. Another example is defensive distillation, which may allow deep neural networks to better deal with evasion attacks (Robust Linear Regression Against <a href='https://www.ccs.neu.edu/home/alina/papers/RobustRegression.pdf'>Training Data Poisoning </a>).\n<a href='https://arxiv.org/pdf/2306.00687.pdf'> More details.</a> "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Model easy to poison",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Model easy to poison",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Model easy to poison",
    "Action": "(Technical) Assess the exposure level of the model used: Some model designs are more commonly used or shared than others and, especially in the ML field; it can be included in their lifecycle to widely share them (e.g. open source sharing). These aspects must be considered in the global application risk analysis. For example, two elements can be distinguished: - Do not reuse models taken directly from the internet without checking them. - Use models for which the threats are clearly identified and for which security controls exist."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Model easy to poison",
    "Action": "(Technical) Assess the exposure level of the model used: Some model designs are more commonly used or shared than others and, especially in the ML field; it can be included in their lifecycle to widely share them (e.g. open source sharing). These aspects must be considered in the global application risk analysis. For example, two elements can be distinguished: - Do not reuse models taken directly from the internet without checking them. - Use models for which the threats are clearly identified and for which security controls exist."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Lack of data for increasing robustness to poisoning",
    "Action": "(Specific ML) Enlarge the training dataset: Using a set of training data expansion techniques (e.g. data augmentation) addresses the lack of data and improves the robustness of the model to poisoning attacks by diluting their impact. It is notable, however, that this security control more specifically addresses poisoning attacks that aim to reduce the performance of the model than those that seek to establish a backdoor. Moreover, one needs to ensure the reliability of the sources used to augment the dataset."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "",
    "Action": "To generate artifical samples, use one of the following tools:\n\nhttps://github.com/snorkel-team/snorkel \nhttps://github.com/QData/TextAttack#augmenting-text-textattack-augment \nhttps://github.com/NVIDIA/DALI"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor data management",
    "Action": "(Organizational) Ensure ML applications comply with data security requirements: \nAs all applications, those using ML must comply with data security requirements to ensure the overall lifecycle of the data they use will be secured (e.g. description of data lifecycle and associated controls, data classification, protection of data at rest and in transit, use of appropriate cryptographic means, data quality controls)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor data management",
    "Action": "(Organizational) Ensure ML applications comply with data security requirements: \nAs all applications, those using ML must comply with data security requirements to ensure the overall lifecycle of the data they use will be secured (e.g. description of data lifecycle and associated controls, data classification, protection of data at rest and in transit, use of appropriate cryptographic means, data quality controls)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Poor data management",
    "Action": "(Organizational) Ensure ML applications comply with data security requirements: \nAs all applications, those using ML must comply with data security requirements to ensure the overall lifecycle of the data they use will be secured (e.g. description of data lifecycle and associated controls, data classification, protection of data at rest and in transit, use of appropriate cryptographic means, data quality controls)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex compromise identification",
    "Action": "Define and monitor indicators for proper functioning of the model: Define dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex compromise identification",
    "Action": "Define and monitor indicators for proper functioning of the model: Define dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "Define anomaly sensors to look at data distribution on day to day basis and alert on variations\nExample: Measure training data variation on daily basis, telemetry for skew/drift"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Use of unsafe data or models (e.g. with transfer learning)",
    "Action": "(Technical) Ensure reliable sources are used: ML is a field in which the use of open-source elements is widespread (e.g., data for training, including labeled ones, models). The trust level of the different sources used should be assessed to prevent using compromise ones. For example: the project wants to use labeled images from a public library. Are the contributors sufficiently trusted to have confidence in the contained images or the quality of their labelling?"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Lack of control for poisoning",
    "Action": "(Specific ML) Integrate poisoning control after the \"model evaluation\" phase: \nBefore moving the model to production and then on a regular basis, the model should be evaluated to ensure it has not been poisoned. This differs from the security control �Use methods to clean the training dataset from suspicious samples�. Indeed, here, it�s the model itself that is evaluated. For example: deep learning classification algorithms can be checked for poisoning using the <a href='https://arxiv.org/pdf/2104.02971.pdf'>STRIP technique</a>. The principle is to disturb the inputs and observe the randomness of the predictions."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "No detection of poisoned samples in the training dataset",
    "Action": "(Technical) Use methods to clean the training dataset from suspicious samples: Removing suspicious samples from the training and testing dataset can help prevent poisoning attacks. Some methods exist to identify those that could cause strange behavior of the algorithm."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "No detection of poisoned samples in the training dataset",
    "Action": "(Technical) Use methods to clean the training dataset from suspicious samples: Removing suspicious samples from the training and testing dataset can help prevent poisoning attacks. Some methods exist to identify those that could cause strange behavior of the algorithm."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Targeted Data Poisoning\n\n\n\n\n\n\n\n\n\n\nIndiscriminate Data Poisoning",
    "Description": "The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase[1]. In targeted poisoning attacks, the attacker wants to misclassify specific examples to cause specific actions to be taken or omitted.\nExample: Submitting AV software as malware to force its misclassification as malicious and eliminate the use of targeted AV software on client systems.\n\nGoal is to ruin the quality/integrity of the data set being attacked. Many datasets are public/untrusted/uncurated, so this creates additional concerns around the ability to spot such data integrity violations in the first place. Training on unknowingly compromised data is a garbage-in/garbage-out situation. Once detected, triage needs to determine the extent of data that has been breached and quarantine/retrain.\nExample: A company scrapes a well-known and trusted website for oil futures data to train their models. The data provider�s website is subsequently compromised via SQL Injection attack. The attacker can poison the dataset at will and the model being trained has no notion that the data is tainted.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Poisoning",
    "Sub-Threat": "Label modification",
    "Description": "An attack in which the attacker corrupts the labels of training data.\nThis sub-threat is specific to Supervised Learning.",
    "Vulnerability (consequence)": "Use of unreliable sources to label data",
    "Action": "(Technical) Ensure reliable sources are used: \nML is a field in which the use of open-source elements is widespread (e.g., data for training, including labeled ones, models). The trust level of the different sources used should be assessed to prevent using compromise ones. For example: the project wants to use labeled images from a public library. Are the contributors sufficiently trusted to have confidence in the contained images or the quality of their labelling?"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Data documentation",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to demonstrate proactive behaviour in case of data breach",
    "Action": "ICO 3.6 - Document clear audit trails of how personal data is moved and stored from one location to another during the training and testing phase to prevent a security breach where personal data is accidentally lost.\n- You should keep an up-to-date inventory of all AI systems to allow you to have a baseline understanding of where potential incidents could occur.\n- You should document security processes and make it freely available for all those involved in the building and deployment of AI systems. This should include processes to report security breaches, and who is responsible for handling and managing them as part of an AI incident response plan. An AI incident response plan should include guidance on how to quickly address any failures or attacks that occur, who responds when an incident occurs, and how they communicate the incident to other parts of the organisation."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Data documentation",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to demonstrate proactive behaviour in case of data breach",
    "Action": "ICO 3.6 - Document clear audit trails of how personal data is moved and stored from one location to another during the training and testing phase to prevent a security breach where personal data is accidentally lost.\n- You should keep an up-to-date inventory of all AI systems to allow you to have a baseline understanding of where potential incidents could occur.\n- You should document security processes and make it freely available for all those involved in the building and deployment of AI systems. This should include processes to report security breaches, and who is responsible for handling and managing them as part of an AI incident response plan. An AI incident response plan should include guidance on how to quickly address any failures or attacks that occur, who responds when an incident occurs, and how they communicate the incident to other parts of the organisation."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Data documentation",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to demonstrate proactive behaviour in case of data breach",
    "Action": "ICO 3.6 - Document clear audit trails of how personal data is moved and stored from one location to another during the training and testing phase to prevent a security breach where personal data is accidentally lost.\n- You should keep an up-to-date inventory of all AI systems to allow you to have a baseline understanding of where potential incidents could occur.\n- You should document security processes and make it freely available for all those involved in the building and deployment of AI systems. This should include processes to report security breaches, and who is responsible for handling and managing them as part of an AI incident response plan. An AI incident response plan should include guidance on how to quickly address any failures or attacks that occur, who responds when an incident occurs, and how they communicate the incident to other parts of the organisation."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Data documentation",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to demonstrate proactive behaviour in case of data breach",
    "Action": "ICO 3.6 - Document clear audit trails of how personal data is moved and stored from one location to another during the training and testing phase to prevent a security breach where personal data is accidentally lost.\n- You should keep an up-to-date inventory of all AI systems to allow you to have a baseline understanding of where potential incidents could occur.\n- You should document security processes and make it freely available for all those involved in the building and deployment of AI systems. This should include processes to report security breaches, and who is responsible for handling and managing them as part of an AI incident response plan. An AI incident response plan should include guidance on how to quickly address any failures or attacks that occur, who responds when an incident occurs, and how they communicate the incident to other parts of the organisation."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Data documentation",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to demonstrate proactive behaviour in case of data breach",
    "Action": "ICO 3.6 - Document clear audit trails of how personal data is moved and stored from one location to another during the training and testing phase to prevent a security breach where personal data is accidentally lost.\n- You should keep an up-to-date inventory of all AI systems to allow you to have a baseline understanding of where potential incidents could occur.\n- You should document security processes and make it freely available for all those involved in the building and deployment of AI systems. This should include processes to report security breaches, and who is responsible for handling and managing them as part of an AI incident response plan. An AI incident response plan should include guidance on how to quickly address any failures or attacks that occur, who responds when an incident occurs, and how they communicate the incident to other parts of the organisation."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Lack of malicious behaviours discover",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Difficult to discover runtime misbheaviours or malicious behaviours against the model",
    "Action": "ICO 4.3 - Document and define technical and organisational measures that will reduce security risks to detect and correct security vulnerabilities.\n- You should assess the trade-off between explainability of your model and the risk of a security breach.\n- You should proactively monitor your AI system and investigate any anomalies.\n- You should introduce real-time monitoring techniques that can detect anomalies (eg <a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-governance-via-apis/'>'rate limiting'</a> which reduces the number of queries that can be performed by a particular user in a given time limit).\n- You should deny anonymous use of your AI system by implementing processes that require user identity.\n- You could employ someone to regularly debug your model."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "N.A.",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Model less robust and possibly not policy-compliant",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/continuous-rai-validator/'>(Pattern) Continuous ethical validator</a>\n- Choose the architecture and deployment option to realize the pattern"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "N.A.",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Model less robust and possibly not policy-compliant",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/continuous-rai-validator/'>(Pattern) Continuous ethical validator</a>\n- Choose the architecture and deployment option to realize the pattern"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "N.A.",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Model less robust and possibly not policy-compliant",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/continuous-rai-validator/'>(Pattern) Continuous ethical validator</a>\n- Choose the architecture and deployment option to realize the pattern"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "N.A.",
    "Description": "Measures which not address a specific threat but contribute to make the model more robust and policy-compliant",
    "Vulnerability (consequence)": "Model less robust and possibly not policy-compliant",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/continuous-rai-validator/'>(Pattern) Continuous ethical validator</a>\n- Choose the architecture and deployment option to realize the pattern"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Lack of detection of abnormal inputs",
    "Action": "(Specific ML) Implement tools to detect if a data point is an adversarial example or not\nInput-based detection tools can be of interest to identify whether a given input has been modified by an attacker or not. One example, in the case of Deep Neural Networks (DNNs), is to add a neural subnetwork to an architecture trained to detect adversarial examples.\n<a href='https://arxiv.org/pdf/1709.05583.pdf'>More details</a>"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Lack of detection of abnormal inputs",
    "Action": "(Specific ML) Implement tools to detect if a data point is an adversarial example or not\nInput-based detection tools can be of interest to identify whether a given input has been modified by an attacker or not. One example, in the case of Deep Neural Networks (DNNs), is to add a neural subnetwork to an architecture trained to detect adversarial examples.\n<a href='https://arxiv.org/pdf/1709.05583.pdf'>More details</a> "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Lack of detection of abnormal inputs",
    "Action": "(Specific ML) Implement tools to detect if a data point is an adversarial example or not\nInput-based detection tools can be of interest to identify whether a given input has been modified by an attacker or not. One example, in the case of Deep Neural Networks (DNNs), is to add a neural subnetwork to an architecture trained to detect adversarial examples.\n<a href='https://arxiv.org/pdf/1709.05583.pdf'>More details</a>"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Lack of detection of abnormal inputs",
    "Action": "(Organizational) Include ML applications into detection and response to security incident processes\nAs all applications, those using ML must be integrated in global processes for detection and incident response. This implies collecting the appropriate logs, configuring relevant detection use cases to detect attacks on the application, and giving the keys to incident response team for efficient response."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Lack of detection of abnormal inputs",
    "Action": "(Organizational) Include ML applications into detection and response to security incident processes\nAs all applications, those using ML must be integrated in global processes for detection and incident response. This implies collecting the appropriate logs, configuring relevant detection use cases to detect attacks on the application, and giving the keys to incident response team for efficient response."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Poor consideration of evasion attacks in the model design implementation",
    "Action": "(Specific ML) Choose and define a more resilient model design\nSome model designs can be more robust than others against attacks. For instance, ensemble methods like bagging can mitigate the impact of poisoning (during the training phase). Another example is defensive distillation, which may allow deep neural networks to better deal with evasion attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake. Following is a more detailed explanation.\n\nIn case Targeted miscalssification, attackers generate a sample that is not in the input class of the target classifier but gets classified by the model as that particular input class. The adversarial sample can appear like random noise to human eyes but attackers have some knowledge of the target machine learning system to generate a white noise that is not random but is exploiting some specific aspects of the target model. The adversary gives an input sample that is not a legitimate sample, but the target system classifies it as a legitimate class.\n\nIn case Random miscalssification, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future [3].\n\nIn case Confidence reduction, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future.\nAn attacker can craft inputs to reduce the confidence level of correct classification, especially in high-consequence scenarios. This can also take the form of a large number of false positives meant to overwhelm administrators or monitoring systems with fraudulent alerts indistinguishable from legitimate alerts.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "(Specific ML) Add some adversarial examples to the training dataset\nInclude adversarial examples to the algorithm's training to enable it to be more resilient to such attacks. Depending on the application domain and ambient conditions, such training could be done continuously.\nThere exist many techniques to generate these samples; most common is Fast Gradient Sign Method (FGSM) and all details, including other alternative techniques, can be found <a href='https://arxiv.org/abs/2303.06302'>here</a>. "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake. Following is a more detailed explanation.\n\nIn case Targeted miscalssification, attackers generate a sample that is not in the input class of the target classifier but gets classified by the model as that particular input class. The adversarial sample can appear like random noise to human eyes but attackers have some knowledge of the target machine learning system to generate a white noise that is not random but is exploiting some specific aspects of the target model. The adversary gives an input sample that is not a legitimate sample, but the target system classifies it as a legitimate class.\n\nIn case Random miscalssification, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future [3].\n\nIn case Confidence reduction, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future.\nAn attacker can craft inputs to reduce the confidence level of correct classification, especially in high-consequence scenarios. This can also take the form of a large number of false positives meant to overwhelm administrators or monitoring systems with fraudulent alerts indistinguishable from legitimate alerts.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "To generate adversarial examples, use one of the following tools:\n\nhttps://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks#2-poisoning-attacks\n\nhttps://github.com/pralab/secml\n\nhttps://github.com/Azure/counterfit/\n\nhttps://github.com/cleverhans-lab/cleverhans\n\nhttps://github.com/QData/TextAttack"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake. Following is a more detailed explanation.\n\nIn case Targeted miscalssification, attackers generate a sample that is not in the input class of the target classifier but gets classified by the model as that particular input class. The adversarial sample can appear like random noise to human eyes but attackers have some knowledge of the target machine learning system to generate a white noise that is not random but is exploiting some specific aspects of the target model. The adversary gives an input sample that is not a legitimate sample, but the target system classifies it as a legitimate class.\n\nIn case Random miscalssification, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future [3].\n\nIn case Confidence reduction, the attacker�s target classification can be anything other than the legitimate source classification. The attack generally involves injection of noise randomly into the source data being classified to reduce the likelihood of the correct classification being used in the future.\nAn attacker can craft inputs to reduce the confidence level of correct classification, especially in high-consequence scenarios. This can also take the form of a large number of false positives meant to overwhelm administrators or monitoring systems with fraudulent alerts indistinguishable from legitimate alerts.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "To test against adversarial attacks, use one of the following tools:\n\nhttps://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks#2-poisoning-attacks\n\nhttps://github.com/pralab/secml\n\nhttps://github.com/Azure/counterfit/\n\nhttps://github.com/cleverhans-lab/cleverhans\n\nhttps://github.com/QData/TextAttack"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Source/Target misclassification",
    "Description": "This is characterized as an attempt by an attacker to get a model to return their desired label for a given input. This usually forces a model to return a false positive or false negative. The end result is a subtle takeover of the model�s classification accuracy, whereby an attacker can induce specific bypasses at will.\nWhile this attack has a significant detrimental impact to classification accuracy, it can also be more time-intensive to carry out given that an adversary must not only manipulate the source data so that it is no longer labeled correctly, but also labeled specifically with the desired fraudulent label. These attacks often involve multiple steps/attempts to force misclassification [3]. If the model is susceptible to transfer learning attacks which force targeted misclassification, there may be no discernable attacker traffic footprint as the probing attacks can be carried out offline.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "Reactive/Defensive Detection Actions\n- Implement a minimum time threshold between calls to the API providing classification results. This slows down multi-step attack testing by increasing the overall amount of time required to find a success perturbation.\nProactive/Protective Actions\nThis attack can be mitigated by including adversarial examples to the algorithm's training to enable it to be more resilient to such attacks. Depending on the application domain and ambient conditions, such training could be done continuously.\nThere exist many techniques to generate these samples; most common is Fast Gradient Sign Method (FGSM) and all details, including other alternative techniques, can be found <a href='https://arxiv.org/abs/2303.06302'>here</a>. "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Source/Target misclassification",
    "Description": "This is characterized as an attempt by an attacker to get a model to return their desired label for a given input. This usually forces a model to return a false positive or false negative. The end result is a subtle takeover of the model�s classification accuracy, whereby an attacker can induce specific bypasses at will.\nWhile this attack has a significant detrimental impact to classification accuracy, it can also be more time-intensive to carry out given that an adversary must not only manipulate the source data so that it is no longer labeled correctly, but also labeled specifically with the desired fraudulent label. These attacks often involve multiple steps/attempts to force misclassification [3]. If the model is susceptible to transfer learning attacks which force targeted misclassification, there may be no discernable attacker traffic footprint as the probing attacks can be carried out offline.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "To generate adversarial examples, use one of the following tools:\n\nhttps://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks#2-poisoning-attacks\n\nhttps://github.com/pralab/secml\n\nhttps://github.com/Azure/counterfit/\n\nhttps://github.com/cleverhans-lab/cleverhans\n\nhttps://github.com/QData/TextAttack"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Source/Target misclassification",
    "Description": "This is characterized as an attempt by an attacker to get a model to return their desired label for a given input. This usually forces a model to return a false positive or false negative. The end result is a subtle takeover of the model�s classification accuracy, whereby an attacker can induce specific bypasses at will.\nWhile this attack has a significant detrimental impact to classification accuracy, it can also be more time-intensive to carry out given that an adversary must not only manipulate the source data so that it is no longer labeled correctly, but also labeled specifically with the desired fraudulent label. These attacks often involve multiple steps/attempts to force misclassification [3]. If the model is susceptible to transfer learning attacks which force targeted misclassification, there may be no discernable attacker traffic footprint as the probing attacks can be carried out offline.",
    "Vulnerability (consequence)": "Lack of training based on adversarial attacks",
    "Action": "To test against adversarial attacks, use one of the following tools:\n\nhttps://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks#2-poisoning-attacks\n\nhttps://github.com/pralab/secml\n\nhttps://github.com/Azure/counterfit/\n\nhttps://github.com/cleverhans-lab/cleverhans\n\nhttps://github.com/QData/TextAttack"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Use a widely known model allowing the attacker to study it",
    "Action": "(Specific ML) Use less easily transferable models\nThe transferability property can be used to force adversarial examples from a substitution model to evade another. The ease of transferring an adversarial example from a model to another depends on the family of algorithms. One possible defense is thus to choose an algorithm family that is less sensitive to the transferability of adversarial examples."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Use a widely known model allowing the attacker to study it",
    "Action": "(Technical) Assess the exposure level of the model used\nSome model designs are more commonly used or shared than others and, especially in the ML field; it can be included in their lifecycle to widely share them (e.g. open source sharing). These aspects must be considered in the global application risk analysis. For example, two elements can be distinguished: \n- Do not reuse models taken directly from the internet without checking them. \n- Use models for which the threats are clearly identified and for which security controls exist."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "N.A.",
    "Description": "A type of attack in which the attacker works on the ML algorithm's inputs to find small perturbations leading to large modification of its outputs (e.g. decision errors). It is as if the attacker created an optical illusion for the algorithm. Such modified inputs are often called adversarial examples.\nExample: the projection of images on a house could lead the algorithm of an autonomous car to take the decision to suddenly make it brake.",
    "Vulnerability (consequence)": "Use a widely known model allowing the attacker to study it",
    "Action": "(Technical) Assess the exposure level of the model used\nSome model designs are more commonly used or shared than others and, especially in the ML field; it can be included in their lifecycle to widely share them (e.g. open source sharing). These aspects must be considered in the global application risk analysis. For example, two elements can be distinguished: \n- Do not reuse models taken directly from the internet without checking them. \n- Use models for which the threats are clearly identified and for which security controls exist."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Use of adversarial examples crafted in white or grey box conditions (e.g. FGSM�)",
    "Description": "In some cases, the attacker has access to information (model, model parameters, etc.) that can allow him to directly build adversarial examples. One example is to directly use the model's gradient to find the best perturbation to add to the input data to evade the model.",
    "Vulnerability (consequence)": "Too much information available on the model",
    "Action": "(Specific ML) Reduce the available information about the model\nThis defense consists of limiting the information about the model when it is not necessary. More precisely, it aims at taking the necessary actions in order to reduce the information available on the model such as information on the training data set or any other information that could be used by an attacker (e.g., not publishing the model in open source). Of course, there is a trade-off between security and the fact that stakeholders (e.g., users, ML teams) sometimes want open source models. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Use of adversarial examples crafted in white or grey box conditions (e.g. FGSM�)",
    "Description": "In some cases, the attacker has access to information (model, model parameters, etc.) that can allow him to directly build adversarial examples. One example is to directly use the model's gradient to find the best perturbation to add to the input data to evade the model.",
    "Vulnerability (consequence)": "Too much information available on the model",
    "Action": "(Specific ML) Reduce the available information about the model\nThis defense consists of limiting the information about the model when it is not necessary. More precisely, it aims at taking the necessary actions in order to reduce the information available on the model such as information on the training data set or any other information that could be used by an attacker (e.g., not publishing the model in open source). Of course, there is a trade-off between security and the fact that stakeholders (e.g., users, ML teams) sometimes want open source models. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Use of adversarial examples crafted in white or grey box conditions (e.g. FGSM�)",
    "Description": "In some cases, the attacker has access to information (model, model parameters, etc.) that can allow him to directly build adversarial examples. One example is to directly use the model's gradient to find the best perturbation to add to the input data to evade the model.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Use of adversarial examples crafted in white or grey box conditions (e.g. FGSM�)",
    "Description": "In some cases, the attacker has access to information (model, model parameters, etc.) that can allow him to directly build adversarial examples. One example is to directly use the model's gradient to find the best perturbation to add to the input data to evade the model.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evasion",
    "Sub-Threat": "Use of adversarial examples crafted in white or grey box conditions (e.g. FGSM�)",
    "Description": "In some cases, the attacker has access to information (model, model parameters, etc.) that can allow him to directly build adversarial examples. One example is to directly use the model's gradient to find the best perturbation to add to the input data to evade the model.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Too much information available on the model",
    "Action": "(Specific ML) Reduce the available information about the model\nThis defense consists of limiting the information about the model when it is not necessary. More precisely, it aims at taking the necessary actions in order to reduce the information available on the model such as information on the training data set or any other information that could be used by an attacker (e.g., not publishing the model in open source). Of course, there is a trade-off between security and the fact that stakeholders (e.g., users, ML teams) sometimes want open source models. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Too much information available on the model",
    "Action": "(Specific ML) Reduce the available information about the model\nThis defense consists of limiting the information about the model when it is not necessary. More precisely, it aims at taking the necessary actions in order to reduce the information available on the model such as information on the training data set or any other information that could be used by an attacker (e.g., not publishing the model in open source). Of course, there is a trade-off between security and the fact that stakeholders (e.g., users, ML teams) sometimes want open source models. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because the ML application was not integrated into process for integrating security into projects",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because the ML application was not integrated into process for integrating security into projects",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because the ML application was not integrated into process for integrating security into projects",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable components (among the whole supply chain)",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable components (among the whole supply chain)",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable components (among the whole supply chain)",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable components (among the whole supply chain)",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable components (among the whole supply chain)",
    "Action": "(Technical) Check the vulnerabilities of the components used so that they have an appropriate security level\nDuring the lifecycle of an ML algorithm, several components (such as software, programming libraries or even other models) are used to complete the project. Security checks have to be carried out to ensure that these components offer an adequate level of security. Moreover, some mechanisms need to be used to prevent tampering with the components used. For example: if an open-source library is to be used, code reviews or check for public vulnerabilities on it can be done."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Use of vulnerable models\nOwing to large resources (data + computation) required to train algorithms, the current practice is to reuse models trained by large corporations and modify them slightly for task at hand (e.g: ResNet is a popular image recognition model from Microsoft). These models are curated in a Model Zoo (Caffe hosts popular image recognition models). In this attack, the adversary attacks the models hosted in Caffe, thereby poisoning the well for anyone else. [1]",
    "Action": "- Minimize 3rd-party dependencies for models and data where possible.\n- Incorporate these dependencies into your threat modeling process.\n- Leverage strong authentication, access control and encryption between 1st/3rd-party systems."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profile making the request. For example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. However, it remains notable that in many cases, research has shown that minimal information is sufficient to mount attacks."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of unidentified compromise scenarios",
    "Action": "(Technical) Conduct a risk analysis of the ML application\nA risk analysis of the overall application should be conducted to take into account the specificities of its context, including: \n- The attacker�s motivations \n- The sensitivity of the data handled (e.g. medical or personal and thus subject to regulatory constraints, strategic for the company and should thus be highly protected) \n- The application hosting (e.g. through third parties services, cloud or on premise environments) \n- The model architecture (e.g. its exposition, learning methods) \n- The ML application lifecycle (e.g., model sharing)"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex compromise identification",
    "Action": "(Technical) Define and monitor indicators for proper functioning of the model\nDefine dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex compromise identification",
    "Action": "(Technical) Define and monitor indicators for proper functioning of the model\nDefine dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>. "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Organizational) Ensure ML applications comply with protection policies and are integrated to security operations processes\nAs all applications, those using ML must comply with protection policies (e.g. hardening, anti-malware policy) and be integrated to security operations processes (e.g. vulnerability management, backups)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Organizational) Ensure ML applications comply with protection policies and are integrated to security operations processes\nAs all applications, those using ML must comply with protection policies (e.g. hardening, anti-malware policy) and be integrated to security operations processes (e.g. vulnerability management, backups)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Organizational) Ensure ML applications comply with protection policies and are integrated to security operations processes\nAs all applications, those using ML must comply with protection policies (e.g. hardening, anti-malware policy) and be integrated to security operations processes (e.g. vulnerability management, backups)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Organizational) Ensure ML applications comply with protection policies and are integrated to security operations processes\nAs all applications, those using ML must comply with protection policies (e.g. hardening, anti-malware policy) and be integrated to security operations processes (e.g. vulnerability management, backups)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Organizational) Ensure ML applications comply with protection policies and are integrated to security operations processes\nAs all applications, those using ML must comply with protection policies (e.g. hardening, anti-malware policy) and be integrated to security operations processes (e.g. vulnerability management, backups)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Ensure ML applications comply with security policies\nAs all applications, those using ML must comply with existing security policies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Ensure ML applications comply with security policies\nAs all applications, those using ML must comply with existing security policies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Ensure ML applications comply with security policies\nAs all applications, those using ML must comply with existing security policies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Existence of several vulnerabilities because ML application do not comply with security policies",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "(Technical) Ensure ML applications comply with third parties� security requirements\nAs all applications, those using ML must comply with third parties� security requirements if their context involves suppliers."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "(Technical) Ensure ML applications comply with third parties� security requirements\nAs all applications, those using ML must comply with third parties� security requirements if their context involves suppliers."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "(Technical) Ensure ML applications comply with third parties� security requirements\nAs all applications, those using ML must comply with third parties� security requirements if their context involves suppliers."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "(Technical) Ensure ML applications comply with third parties� security requirements\nAs all applications, those using ML must comply with third parties� security requirements if their context involves suppliers."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "(Technical) Ensure ML applications comply with third parties� security requirements\nAs all applications, those using ML must comply with third parties� security requirements if their context involves suppliers."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk of data breach becomes very low. Anyway, this is a very difficult measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk od data breach becomes very low. Anyway, this is a veryy difficl measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "https://aka.ms/SEALonAML"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk of data breach becomes very low. Anyway, this is a very difficult measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "https://aip360.mybluemix.net/tools#encrypted"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk of data breach becomes very low. Anyway, this is a very difficult measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "https://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "https://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "- If possible, train all sensitive models in-house\n- Catalog training data or ensure it comes from a trusted third party with strong security practices\n- Threat model the interaction between the MLaaS provider and your own systems"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Contract with a low security third party",
    "Action": "- Train all sensitive models in-house"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Compromise of ML application components",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the compromise of a component or developing tool of the ML application.\nExample: compromise of one of the open-source libraries used by the developers to implement the ML algorithm.",
    "Vulnerability (consequence)": "Exploit software dependencies of the ML system\nIn this attack, the attacker does NOT manipulate the algorithms. Instead, exploits software vulnerabilities such as buffer overflows or cross-site scripting[1]. It is still easier to compromise software layers beneath AI/ML than attack the learning layer directly, so traditional security threat mitigation practices detailed in the Security Development Lifecycle are essential.",
    "Action": "Apply Security Development Lifecycle/Operational Security Assurance best practices for non-AI systems <a href='https://www.microsoft.com/en-us/securityengineering/sdl/practices'>here</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Lack of consideration of real-life conditions in training the model",
    "Action": "(Specific ML) Ensure that the model is sufficiently resilient to the environment in which it will operate\nEnsure that the model is sufficiently resilient against the environment in which it will operate. This includes, for instance, ensure that learning process and data are representative enough of the real conditions in which the model will evolve."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Lack of consideration of real-life conditions in training the model",
    "Action": "(Specific ML) Ensure that the model is sufficiently resilient to the environment in which it will operate\nEnsure that the model is sufficiently resilient against the environment in which it will operate. This includes, for instance, ensure that learning process and data are representative enough of the real conditions in which the model will evolve."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Lack of consideration of real-life conditions in training the model",
    "Action": "(Specific ML) Ensure that the model is sufficiently resilient to the environment in which it will operate\nEnsure that the model is sufficiently resilient against the environment in which it will operate. This includes, for instance, ensure that learning process and data are representative enough of the real conditions in which the model will evolve."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Existence of unidentified failure scenarios",
    "Action": "(Technical) Conduct a risk analysis of the ML application\nA risk analysis of the overall application should be conducted to take into account the specificities of its context, including: \n- The attacker�s motivations \n- The sensitivity of the data handled (e.g. medical or personal and thus subject to regulatory constraints, strategic for the company and should thus be highly protected)\n- The application hosting (e.g. through third parties services, cloud or on premise environments) \n- The model architecture (e.g. its exposition, learning methods) \n- The ML application lifecycle (e.g., model sharing"
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex malfunction identification",
    "Action": "(Technical) Define and monitor indicators for proper functioning of the model\nDefine dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Undefined indicators of proper functioning, making complex malfunction identification",
    "Action": "(Technical) Define and monitor indicators for proper functioning of the model\nDefine dashboards of key indicators integrating security indicators (peaks of change in model behavior etc.) to follow-up the proper functioning of the model regarding the business case, in particular to allow rapid identification of anomalies."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>. "
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to ML application failure (e.g. denial of service due to bad input, unavailability due to a handling error). Example: the service level of the support infrastructure of the ML application hosted by a third party is too low compared to the business needs, the application is regularly unavailable. Note that this threat does not consider failure of business use cases (for example, the algorithm fails because it is not accurate enough to handle all real-life situations it is exposed to).",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Apply documentation requirements to AI projects\nAs for all projects, documentation must be produced for AI to preserve knowledge on the choices made during the project phase, the application architecture, its configuration, its maintenance, how to maintain its effectiveness over time and the assumptions made about the model use. This documentation should also include the changes that will be applied, including to the documentation throughout the algorithm's life cycle."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Apply documentation requirements to AI projects\nAs for all projects, documentation must be produced for AI to preserve knowledge on the choices made during the project phase, the application architecture, its configuration, its maintenance, how to maintain its effectiveness over time and the assumptions made about the model use. This documentation should also include the changes that will be applied, including to the documentation throughout the algorithm's life cycle."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Apply documentation requirements to AI projects\nAs for all projects, documentation must be produced for AI to preserve knowledge on the choices made during the project phase, the application architecture, its configuration, its maintenance, how to maintain its effectiveness over time and the assumptions made about the model use. This documentation should also include the changes that will be applied, including to the documentation throughout the algorithm's life cycle."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Apply documentation requirements to AI projects\nAs for all projects, documentation must be produced for AI to preserve knowledge on the choices made during the project phase, the application architecture, its configuration, its maintenance, how to maintain its effectiveness over time and the assumptions made about the model use. This documentation should also include the changes that will be applied, including to the documentation throughout the algorithm's life cycle."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Apply documentation requirements to AI projects\nAs for all projects, documentation must be produced for AI to preserve knowledge on the choices made during the project phase, the application architecture, its configuration, its maintenance, how to maintain its effectiveness over time and the assumptions made about the model use. This documentation should also include the changes that will be applied, including to the documentation throughout the algorithm's life cycle."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Human error",
    "Description": "The different stakeholders of the model can make mistakes that result in a failure or malfunction of ML application. For example, due to lack of documentation, they may use the application in use-cases not initially foreseen.",
    "Vulnerability (consequence)": "Lack of documentation on the ML application",
    "Action": "(Organizational) Include ML applications in asset management processes\nAs all applications, those using ML must be integrated to global processes for asset management to ensure their assets are inventoried, their owners are identified, their information classified."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Denial of service due to inconsistent data or a sponge example",
    "Description": "ML algorithms usually consider input data in a defined format to make their predictions. Thus, a denial of service could be caused by input data whose format is inappropriate. It may also happen that a malicious user of the model constructs an input data (a sponge example) specifically designed to increase the computation time of the model and thus potentially cause a denial of service.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Denial of service due to inconsistent data or a sponge example",
    "Description": "ML algorithms usually consider input data in a defined format to make their predictions. Thus, a denial of service could be caused by input data whose format is inappropriate. It may also happen that a malicious user of the model constructs an input data (a sponge example) specifically designed to increase the computation time of the model and thus potentially cause a denial of service.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Failure or malfunction of ML application",
    "Sub-Threat": "Denial of service due to inconsistent data or a sponge example",
    "Description": "ML algorithms usually consider input data in a defined format to make their predictions. Thus, a denial of service could be caused by input data whose format is inappropriate. It may also happen that a malicious user of the model constructs an input data (a sponge example) specifically designed to increase the computation time of the model and thus potentially cause a denial of service.",
    "Vulnerability (consequence)": "Use of uncontrolled data",
    "Action": "(Techincal) Control all data used by the ML model\nData must be checked to ensure they will suit the model and limit the ingestion of malicious data: \n- Evaluate the trust level of the sources to check it's appropriate in the context of the application \n- Protect their integrity along the whole data supply chain \n- Their format and consistence are verified \n- Their content is checked for anomalies, automatically or manually (e.g. selective human control) \n- In the case of labeled data, the issuer of the label is trusted."
  },
  {
    "Category": "Security",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Neural Net Reprogramming",
    "Sub-Threat": "N.A.",
    "Description": "By means of a specially crafted query from an adversary, Machine learning systems can be reprogrammed to a task that deviates from the creator�s original intent [1].\nExample: Weak access controls on a facial recognition API enabling 3rd parties to incorporate into apps designed to harm Microsoft customers, such as a deep fakes generator.",
    "Vulnerability (consequence)": "Neural Net Easy to Reprogram",
    "Action": "Apply one of the following high-level mitigations:\n\n- Strong client<->server mutual authentication and access control to model interfaces\n- Takedown of the offending accounts.\n- Identify and enforce a service-level agreement for your APIs. Determine the acceptable time-to-fix for an issue once reported and ensure the issue no longer repros once SLA expires."
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "RE",
    "Data type": "General",
    "Local/Global Explanation": "Both",
    "Explanation Goal": "Start considering Explainability from Req. Elicitation",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Elicit also explainability requirements; examples are: \n- Unexpected Prediction: Disagreement with AI: declare the required behaviour in case the AI prediction is unexpected, and/or users disagree with AI�s prediction\n- Expected prediction: declare the required behaviour in case AI�s prediction aligns with users� expectations\n- Differentiate similar instances: due to the consequences of wrong decisions, users sometimes need to discern similar instances or outcomes. For example, a doctor differentiates whether the diagnosis is a benign or malignant tumor\n- Learn from AI: users need to gain knowledge, improve their problem-solving skills, and discover new knowledge\n- Improve the predicted outcome: users seek causal factors to control and improve the predicted outcome\n- Communicate with stakeholders: many critical decision-making processes involve multiple stakeholders, and users need to discuss the decision with them\n- Generate reports: users need to utilize the explanations to perform particular tasks such as report production. For example, a radiologist generates a medical report on a patient�s X-ray image"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "General",
    "Local/Global Explanation": "Both",
    "Explanation Goal": "Have a clear idea about the desired explanation form",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Consider the design of the explanation design: how the final UI should be composed and how to present the information"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Attribution\nPro: Simple and easy to understand; Can answer how and why AI reaches its decisions. \nCons: Illusion of causality, confirmation bias\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "LIME [79], SHAP [67], CAM [105], LRP [17], TCAV [53]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nSimilar Example\nPro: Easy to comprehend, users intuitively verify AI�s decision using analogical reasoning on similar examples\nCons: It does not highlight features within examples to enable users� side-by-side comparison\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Nearest neighbour, CBR [55]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nTypical Example\nPro: Use prototypical instances to show learned representation; Reveal potential problems of the model\nCons: Users may not appreciate the idea of typical cases\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "k-Mediods, MMD-critic [52], Generate prototype [68, 84], CNN prototype [27, 59]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To differentiate between similar instances; To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nCounterfactual Example\nPro: Helpful to identify the differences between the current outcome and another contrastive outcome \nCons: Hard to understand, may cause confusion\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To differentiate between similar instances; To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Inverse classification [58]), MMD-critic [52], Progression [51], Visual [38]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Attribution\nPro: Simple and easy to understand; Can answer how and why AI reaches its decisions. \nCons: Illusion of causality, confirmation bias\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "LIME [79], SHAP [67], CAM [105], LRP [17], TCAV [53]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To control and improve the outcome; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Shape \nPro: Graphical representation, easy to understand the relationship between one feature and prediction\nCons: Lacks feature interaction; Information overload if multiple feature shapes are presented\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To control and improve the outcome; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "PDP [35], ALE [15], GAM [87], SHAP dependence [67]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Interaction\nPro: Show feature-feature interaction\nCons: The diagram on multiple features is difficult to interpret\nComprensibilità della spiegazione: 1"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "PDP [35], ALE [15], GA2M [25], SHAP interaction [67]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nTypical Example\nPro: Use prototypical instances to show learned representation; Reveal potential problems of the model\nCons: Users may not appreciate the idea of typical cases\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "k-Mediods, MMD-critic [52], Generate prototype [68, 84], CNN prototype [27, 59]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Facilitate users� learning, report generation, and communication with other stakeholders",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Rule- based explanation\nDecision Rules/Sets\nPro: Need to carefully balance between completeness and simplicity of explanation\nCons: Trim rules and show on-demand; Highlight local rule clauses related to user�s interested instances\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Facilitate users� learning, report generation, and communication with other stakeholders",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Bayesian Rule Lists [95], LORE [41], Anchors [80]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Comparison; Counterfactual reasoning",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Rule- based explanation\n<a href='http://localhost:5173/Learning#tree'>Decision tree</a>\nPro: Show decision process, explain the differences\nCons: Too much information, complicated to understand\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Tabular",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Comparison; Counterfactual reasoning",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Model distillation [36], Disentangle CNN [102]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Attribution\nPro: Simple and easy to understand; Can answer how and why AI reaches its decisions. \nCons: Illusion of causality, confirmation bias\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "LIME [79], SHAP [67], CAM [105], LRP [17], TCAV [53]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nSimilar Example\nPro: Easy to comprehend, users intuitively verify AI�s decision using analogical reasoning on similar examples\nCons: It does not highlight features within examples to enable users� side-by-side comparison\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Nearest neighbour, CBR [55]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nTypical Example\nPro: Use prototypical instances to show learned representation; Reveal potential problems of the model\nCons: Users may not appreciate the idea of typical cases\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "k-Mediods, MMD-critic [52], Generate prototype [68, 84], CNN prototype [27, 59]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To differentiate between similar instances; To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nCounterfactual Example\nPro: Helpful to identify the differences between the current outcome and another contrastive outcome \nCons: Hard to understand, may cause confusion\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Image/Text",
    "Local/Global Explanation": "Local",
    "Explanation Goal": "To differentiate between similar instances; To control and improve the outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Inverse classification [58]), MMD-critic [52], Progression [51], Visual [38]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Feature- based explanation\nFeature Attribution\nPro: Simple and easy to understand; Can answer how and why AI reaches its decisions. \nCons: Illusion of causality, confirmation bias\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To validate the algorithm outcome",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "LIME [79], SHAP [67], CAM [105], LRP [17], TCAV [53]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Example- based explanation\nTypical Example\nPro: Use prototypical instances to show learned representation; Reveal potential problems of the model\nCons: Users may not appreciate the idea of typical cases\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "To verify the decision; To reveal bias",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "k-Mediods, MMD-critic [52], Generate prototype [68, 84], CNN prototype [27, 59]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Facilitate users� learning, report generation, and communication with other stakeholders",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Rule- based explanation\nDecision Rules/Sets\nPro: Need to carefully balance between completeness and simplicity of explanation\nCons: Trim rules and show on-demand; Highlight local rule clauses related to user�s interested instances\nComprensibilità della spiegazione: 2"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Facilitate users� learning, report generation, and communication with other stakeholders",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Bayesian Rule Lists [95], LORE [41], Anchors [80]"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Design",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Comparison; Counterfactual reasoning",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Rule- based explanation\n<a href='http://localhost:5173/Learning#tree'>Decision tree</a>\nPro: Show decision process, explain the differences\nCons: Too much information, complicated to understand\nComprensibilità della spiegazione: 3"
  },
  {
    "Category": "Explainability",
    "SDLC Phase": "Development",
    "Data type": "Img/Txt",
    "Local/Global Explanation": "Global",
    "Explanation Goal": "Comparison; Counterfactual reasoning",
    "Threat": "",
    "Sub-Threat": "",
    "Description": "",
    "Vulnerability (consequence)": "",
    "Action": "Model distillation [36], Disentangle CNN [102]"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Solutionist Trap",
    "Sub-Threat": "N.A.",
    "Description": "the potential of technology as a means to solve a problem is often overestimated, leading to a higher risk of falling into the Solutionist trap.",
    "Vulnerability (consequence)": "",
    "Action": "To prevent the solutionist trap from happening and to create a more realistic view on whether the problem can indeed be solved or supported with a model, ask yourself the following questions:\n- How would using a predictive model offer a solution for the problem of interest?\n- Which other solutions are available to solve this problem? Why were these solutions not sufficient (anymore)? \n- Why should we use an AI technology to solve this problem?\n- How will we know if our project with AI technology is successful? How will the performance be measured in terms of organisation goals?"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Solutionist Trap",
    "Sub-Threat": "N.A.",
    "Description": "the potential of technology as a means to solve a problem is often overestimated, leading to a higher risk of falling into the Solutionist trap.",
    "Vulnerability (consequence)": "",
    "Action": "Conduct an Artificial Intelligence Impact Assessment (AIIA) by following this <a href='https://static1.squarespace.com/static/5b7877457c9327fa97fef427/t/5c368c611ae6cf01ea0fba53/1547078768062/Artificial+Intelligence+Impact+Assessment+-+English.pdf'>guide</a>.\nTo answer previous questions"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Solutionist Trap",
    "Sub-Threat": "N.A.",
    "Description": "the potential of technology as a means to solve a problem is often overestimated, leading to a higher risk of falling into the Solutionist trap.",
    "Vulnerability (consequence)": "",
    "Action": "Since many key choices are made during this phase that can introduce bias, it�s important to elaborate on these decisions.\nMoreover, during this phase we should establish who our stakeholders are, so we can educate them on the model development cycle. All these findings will be documented so that we can continuously evaluate these decisions during later phases.\nThe discussions and documentation should at least cover the following topics:\n1. Stakeholders: Describe the stakeholders who will be involved in the development and deployment of the model, such as the decision-makers and all the other people (in)directly affected by the system. It is important to involve stakeholders continuously throughout the lifecycle of the algorithm, since they all bring in their own valuable expertise and experiences about the problem. Using these multiple perspectives, we can minimise the risks observed.\n2. Demographic Groups: Define the demographic groups which the AI system is likely to impact. These demographic groups can be formed by (combinations of) sensitive attributes, such as by race, gender, age, or disability. \n- In Figure there is an overview of all the sensitive attributes.\n- The use of personas can also help to better describe the demographic groups. These can be created in collaboration with stakeholders and verified by domain experts who have experience with the population of interest. See also this introduction on <a href='https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them'>personas website </a> for more information about how to create these descriptions.\n3. Fairness Definitions and Metrics: it is recommended considering the fairness of the algorithm as early as possible. More specifically for this stage, we can begin to think about what fairness definition and metrics would be applicable to our algorithm. The sheet \"<a href='https://docs.google.com/spreadsheets/d/1Y3PucJ2PM277dr8qGgpnw3_zRPACOA58IYiGJaQYmPw/edit?usp=sharing'>Tree metrics</a>\" can be used as guidance.\n- Discuss with stakeholders: Which types of mistakes are you more willing to make? This question helps with scoping the bias analysis.\n- Define the fairness definitions and metrics.\n4. Target Variable and Sensitive Attributes: Describe the considerations made for the target variable and the sensitive attributes to avoid construct validity bias. Pay specific attention to describing how these variables will be measured.\n5. Socio-technical Context: Describe the socio-technical environment in which the model will be deployed.\n- How can the sociotechnical context be described in this AI system?\n- How will the working process change due to the new AI system?\n- Are there any relevant regulations, standards or policies that should also be considered?"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Abstraction Traps",
    "Sub-Threat": "N.A.",
    "Description": "The main risks arising during this phase is incorrectly translating the problem to a predictive model. For example, we might oversimplify the problem and the context that we want to solve using algorithms, resulting in an abstraction trap. We can also encounter the portability trap which occurs when we fail to translate the problem into a predictive algorithm, leading to an oversimplified and unrepresentative model.",
    "Vulnerability (consequence)": "",
    "Action": "Discuss with domain experts whether the translation of the problem formulation into a model is modelling the socio-technical context adequately.\nMap out all the possible factors that may have an impact on the algorithm and discuss with domain experts which of these factors can be left out and which can be included. As for the portability trap, make explicit what the differences are between the initial and the new context when reusing a model, and map how this may (negatively) affect the outcomes of the model."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Ripple Effect Trap",
    "Sub-Threat": "N.A.",
    "Description": "The introduction of a new technology in a work process often changes the social dynamics within the system in which it operates. For example, the new technology might impact\nthe power dynamics within the system due to the changed roles and responsibilities of employees. The effects of the changed dynamics can be harmful when not anticipated beforehand.",
    "Vulnerability (consequence)": "",
    "Action": "Pay sufficient attention to how the model can affect the behaviour, perception and expertise of all actors whose work is somehow involved with the newly introduced model, and investigate the relative power dynamics between the actors in the system."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Construct Validity Bias",
    "Sub-Threat": "N.A.",
    "Description": "This complex bias occurs when we use features or target variables that are difficult to measure because they are unobservable constructs, such as socio-economic status\nor fraudulent behaviour, resulting in a mismatch between the real-world problem and the model. When using unobservable constructs as features or predict a target, there is a high risk of ending up with a model that poorly represents these concepts.",
    "Vulnerability (consequence)": "",
    "Action": "Inspecting an AI system for traces of construct validity bias is not an easy task. We need to gain deep insight into how proxy variables (including the target variable) are constructed. A way to mitigate this type of bias is by collecting multiple measures to form the target variable. In addition, the conceptualization framework of construct validity in What is Construct Validity? <a href='https://fairlearn.org/v0.7.0/user_guide/fairness_in_machine_learning.html#what-is-construct-validity'>This website</a> offers a starting point to evaluate the construct validity of your variables."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Historical bias",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias is caused by data that reflects the human biases,\nprejudices and other effects of societal or historical inequities,\nleading to representation and allocation harm.",
    "Vulnerability (consequence)": "",
    "Action": "We identified the following mitigation techniques:\n- Analyse which unjust patterns are embedded in the dataset in collaboration with domain experts. Based on the demographic groups defined in the first phase, further research can be done to investigate how these groups are represented in the dataset.\n- Additionally, since historical bias is often caused by problematic distributions of the features and/or the target variable for the minority groups in the dataset, we could improve these distributions with over and undersampling techniques. \nThese sampling techniques can be used to systematically over or undersample the features and the target for minority groups, for example by assigning more desired target labels to minority groups which increases the probability that they receive a desired outcome by the model. For more information, see the mitigation techniques for representation bias.\n- Inspect the decisions and interventions that result from the outcomes of the model, especially when they have a punitive nature. Discuss with stakeholders how to extend the set of decisions and interventions with more assistive actions. For example, for loan eligibility models, extend the decision space with options to offer different interest rates and payment terms. <a href='https://www.annualreviews.org/doi/10.1146/annurev-statistics-042720-125902'>More details</a>"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "N.A.",
    "Description": "Statistical bias stems from a mismatch between the sample used to train a predictive model and the world as it currently is. <a href='https://doi.org/10.1145/3465416.3483305'>More details</a>",
    "Vulnerability (consequence)": "",
    "Action": "/"
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "Representation bias",
    "Description": "This type of bias occurs when some groups are underrepresented in the dataset, leading the model to not generalize well for these groups and eventually causing <a href='https://github.com/pyladiesams/ml-fairness-beginner-nov2021/blob/master/workshop/measuringgroupfairness.ipynb'>quality-of-service harm </a>. The underrepresentation of demographic groups in the dataset is often caused by selection bias, where a sample is selected as dataset in a way that is  irreflective of the real-world distribution.",
    "Vulnerability (consequence)": "",
    "Action": "To prevent representation bias:\n- Be aware of your own blind spots as a data scientist and consult domain experts;\n- Develop the model with a diverse team;\n- Ensure your dataset contains sufficient instances of minority groups. \nThe population selected for the algorithm�s training should have similar distributions and proportions for all subgroups and for each protected attribute. Data visualisation techniques can be helpful to gain more insight into the differences between the data of the minority groups and the majority group."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "Representation bias",
    "Description": "This type of bias occurs when some groups are underrepresented in the dataset, leading the model to not generalize well for these groups and eventually causing <a href='https://github.com/pyladiesams/ml-fairness-beginner-nov2021/blob/master/workshop/measuringgroupfairness.ipynb'>quality-of-service harm </a>. The underrepresentation of demographic groups in the dataset is often caused by selection bias, where a sample is selected as dataset in a way that is  irreflective of the real-world distribution.",
    "Vulnerability (consequence)": "",
    "Action": "To mitigate representation bias:\n- Collect additional data to mitigate the dataset imbalance;\n- Find ways to deal with sampling errors. Consider pre-processing mitigation algorithms like methods to reweigh the instances of the dataset such that people from unprivileged groups with favourable labels get assigned higher weights while privileged people with favourable labels are assigned lower weights. \n- Use sampling techniques to obtain balanced dataset, for example with oversampling, undersampling and stratified sampling. These sampling techniques are only conducted on the training set: the validation and test set remain untouched. For a Python package about dealing with imbalanced data, we recommend the Imbalanced-Learn  package and reading through the corresponding <a href='https://jmlr.org/papers/v18/16-365.html'>paper</a>."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "Measurement bias",
    "Description": "Measurement bias occurs when the data contains systematic\npatterns of measurement errors which are greater for some groups\nthan for others, leading to a greater magnitude of errors for these\ngroups and resulting in quality-of-service harm.",
    "Vulnerability (consequence)": "",
    "Action": "To mitigate the risk of measurement bias, re-evaluate the measurement or annotation process from a more context-aware perspective. Consult domain experts to provide more background information about all the factors that are related to the target variable, and select together which features are less prone to measurement errors. See also the proposed  mitigation techniques of Omitted Variable Bias. If there is some information available about the ground truth of the data, then this information will be valuable in assessing the  model for measurement bias."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "Measurement bias",
    "Description": "Measurement bias occurs when the data contains systematic\npatterns of measurement errors which are greater for some groups\nthan for others, leading to a greater magnitude of errors for these\ngroups and resulting in quality-of-service harm.",
    "Vulnerability (consequence)": "",
    "Action": "DATASHEET: It is important to describe the choices behind the data collection and sampling process in detail, as this gives insight into the distribution of the sampled population and can be used to find traces of representation bias. The <a href='https://doi.org/10.1145/3458723'>datasheets</a> introduced by Gebru et al.  can be useful here: this tool helps with documenting the key information about the dataset). The datasheet includes information about the following dataset properties: \n- The source of the dataset;\n- The timeframe over which the data was collected;\n- The collection, aggregation or curation process of the dataset, which also includes the used software, hardware, or infrastructure to collect and process the data;\n- The (pre)processing techniques used to prepare the dataset for the model training phase."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Statistical bias",
    "Sub-Threat": "Measurement bias",
    "Description": "Measurement bias occurs when the data contains systematic\npatterns of measurement errors which are greater for some groups\nthan for others, leading to a greater magnitude of errors for these\ngroups and resulting in quality-of-service harm.",
    "Vulnerability (consequence)": "",
    "Action": "FEATURE REVIEW: In the Feature Review, the relevant characteristics of the dataset features are documented and evaluated for potential links to direct or indirect bias. This descriptive analysis is compiled in collaboration with domain experts and end users to gain more insight about the features and business rules that shape the AI system. Specifically, the Feature Overview helps with determining: \n- Which features have a larger risk for direct or indirect bias;\n- To which sensitive attributes each feature is linked. These sensitive attributes help with determining the demographic groups who will be evaluated during the bias analysis.\nThe Feature Review is one of the key documents of the Fairness Pipeline and the bias analysis (which should be done through the dedicated sheet, if not aready done in RE phase). Therefore, make sure to schedule sufficient time with stakeholders to fill in the document.\nFor each feature, include the following information:\n- What is the source of this feature?\n- What does this feature tell us? How can it be interpreted?\n- How is the information behind the feature currently used in the work process?\n- Is there any legal, academic or common-sense justification for the link between this feature and the target variable? If so, how credible, strong and actual is this support?\n- To which protected attributes is it linked? (See Figure above for an overview of the protected attributes)\n- To which demographic groups does this feature link?\n- Will this feature be evaluated during the bias analysis?\nIf so, will it be analysed for indirect or direct bias?\nMake sure to continuously update the information in this overview, as it will be often used in the subsequent phases to describe the results of the applied Fairness Pipeline tools."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Learning Bias",
    "Sub-Threat": "N.A.",
    "Description": "Learning bias occurs when the model prioritizes some objective, e.g., accuracy, that damages a fairness-related outcome.",
    "Vulnerability (consequence)": "",
    "Action": "Mitigation techniques should target the defined learning objectives and associated learning processes. Moreover, since this bias can amplify performance disparities on underrepresented groups, it is important to ensure there is no representation bias. With a more representative and balanced dataset, the model will be less prone to only preserving information about the majority group."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Aggregation Bias",
    "Sub-Threat": "N.A.",
    "Description": "There is a greater risk for aggregation bias when a single model is applied on data consisting of (demographic) groups with distinct distributions that should be treated differently. In other words, the model wrongly assumes that the data distribution is homogeneous.",
    "Vulnerability (consequence)": "",
    "Action": "- Adjust the objective function to include the group differences in the data. In some cases, incorporating information about group differences into the design of the model can lead to a simpler function that the model can learn, which in turn can improve performance across groups. A branch to look into are coupled learning methods such as multitask learning, which modify the parameters of the <a href='https://doi.org/10.1145/3465416.3483305'>model objective</a> to also consider the group differences .\n- Adjust the training data to fit the objective function better, for example with data transformation techniques such as the <a href='https://proceedings.mlr.press/v28/zemel13.html'>Fair Representation Learning method</a>."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Omitted Variable Bias",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias occurs when a single or multiple important features are left out of the <a href='https://doi.org/10.1145/3194770.3194776'> model </a> .",
    "Vulnerability (consequence)": "",
    "Action": "To mitigate this risk, use feature importance methods to evaluate the relationship between each feature and the target variable.\nThe <a href='https://christophm.github.io/interpretable-ml-book/pdp.html'>Permutation Feature Importance method</a> is one of the techniques that could be used . This method measures the increase in the prediction error of the model after permuting the feature�s values. Consult this source for a more detailed explanation about Permutation Feature Importance."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Omitted Variable Bias",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias occurs when a single or multiple important features are left out of the <a href='https://doi.org/10.1145/3194770.3194776'> model </a>.",
    "Vulnerability (consequence)": "",
    "Action": "It is crucial to diagnose the model and the generated outcomes for presence of bias and to grasp why the model arrived at its outcomes. The more transparent the model is, the easier this process will be, so make sure to look into methods for increasing transparency and understandability of the model.\nWhen prioritizing transparency and understandability, the most straightforward option is to choose human-interpretable models, such as regression models, <a href='http://localhost:5173/Learning#tree'>decision trees</a>, Na�ve Bayes Classifiers and K-Nearest Neighbours. The open-source book �Interpretable Machine Learning� by Christoph Molnar (Learning�) is a recommended read for learning more about how to select interpretable models, or how to implement model-agnostic methods in more complex algorithmes to understand the decisions generated by the model."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Omitted Variable Bias",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias occurs when a single or multiple important features are left out of the <a href='https://doi.org/10.1145/3194770.3194776'> model </a>.",
    "Vulnerability (consequence)": "",
    "Action": "Feature Importance Methods: After training the model, feature importance methods such as Permutation Feature Importance or SHAP can be used to calculate the relative importance of each feature. These results should be communicated with the domain exports and end users to evaluate whether the most predictive values are proxies for indirect bias and whether these features are indeed essential for solving the problem in the real world.\nIf stakeholders do not recognize the most predictive features being essential for the real-world problem, it is advisable to re-evaluate the features and the functioning of the model. By doing so, the risk that the model produces harmful and erroneous outcomes is minimized. Finally, make sure to register the feature importance results in the Feature Review."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Omitted Variable Bias",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias occurs when a single or multiple important features are left out of the<a href='https://doi.org/10.1145/3194770.3194776'> model </a>.",
    "Vulnerability (consequence)": "",
    "Action": "Explore Counterfactuals: With counterfactuals, we change the values of sensitive attributes (or features linking to sensitive information) and observe whether the model outcome changes positively or negatively. Suppose, for example, we inspect a model that determines eligibility for a life insurance, and we include a feature that indirectly links to nationality, such as history of foreign travel. If changing the value of this feature from Italy to Lebanon increases the insurance rate significantly, the model might be biased against\nnationality, ethnicity or migration background. To experiment with counterfactuals, Google's What-If Tool offers a wide range of possibilities to probe your model and investigate\nwhich counterfactuals are present. The insights obtained from exploring counterfactuals can, for example, help with determining whether the decision space of the model should be extended with more interventions."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Wrong assumptions behind Evaluations",
    "Sub-Threat": "N.A.",
    "Description": "Generally, model evaluations are based on three underlying assumptions:\n- Decisions can be evaluated as an aggregation of separately evaluated individual decisions. This includes assuming that outcomes are not affected by the decisions for others, an\nassumption known as no interference.\n- All individuals can be considered symmetrically, i.e., identically. This assumes, for example, that the harm of denying a loan to someone who could repay is equal across all people.\n- Decisions are evaluated simultaneously. This means that they are evaluated in a batch as opposed to serially, and therefore they do not consider potentially important temporal dynamics.",
    "Vulnerability (consequence)": "",
    "Action": "Evaluate you decisions w.r.t. the Threat description."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evaluation bias",
    "Sub-Threat": "N.A.",
    "Description": "Evaluation bias occurs when the evaluation metrics are inappropriate for the model and dataset, hereby disguising the model�s performance for smaller-sized demographic groups.",
    "Vulnerability (consequence)": "",
    "Action": "Use disaggregated evaluation metrics on smaller groups of data to gain more insight in the model�s performance on minority groups. These subgroup metrics can also be used to compare the performance of groups with each other to find performance disparities, which often indicate various kinds of biases that we discussed. For example, both accuracy and precision could be calculated and compared for the self-defined groups. \nKeep in mind that selecting smaller-sized groups and metrics is always application-dependent, and it often requires intersectional analysis and privacy considerations. Therefore, input should be sought from domain experts and affected populations who understand the usage and consequences of the <a href='https://doi.org/10.1145/3465416.3483305'>model</a>. Tools such as the <a href='http://localhost:5173/Learning#tree'>Fairness tree</a> can assist in selecting these appropriate metrics.\nBesides reporting the performance of models on more granular subsets of data, we also recommend to closely inspect the data distribution for dataset imbalances that cause the model to underperform for certain subpopulations. If the subgroup evaluation metrics indicate a disparity in performance, then considering new ways to generate more representative data could be helpful to mitigate the evaluation bias, which are mentioned at the solution strategies for representation bias."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evaluation bias",
    "Sub-Threat": "N.A.",
    "Description": "Evaluation bias occurs when the evaluation metrics are inappropriate for the model and dataset, hereby disguising the model�s performance for smaller-sized demographic groups.",
    "Vulnerability (consequence)": "",
    "Action": "Bias Analysis: It is one of the most important actions of the Fairness Pipeline. Here, fairness metrics are used to compare the model�s performance across demographic groups and to find the groups for which the model is substantially underperforming, hereby possibly indicating bias. \nAs the bias analysis is a very intense and elaborate process, consult the dedicated Excel sheet. Based on the obtained results from the bias analysis, it is recommended to return to previous phases to solve the underlying problems causing the bias."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Evaluation bias",
    "Sub-Threat": "N.A.",
    "Description": "Evaluation bias occurs when the evaluation metrics are inappropriate for the model and dataset, hereby disguising the model�s performance for smaller-sized demographic groups.",
    "Vulnerability (consequence)": "",
    
    "Action": "Bias Scenario Simulations: An useful exercise within the bias analysis is to create realistic simulations of unfavorable outcomes for demographic groups and to discuss the follow-up steps of these results with stakeholders.\nThe simulations can be made using different <a href='http://localhost:5173/Learning#matrix-containero'>confusion matrix </a> outcomes for demographic groups, thereby highlighting how the model performs differently for advantaged versus disadvantaged groups. Simulating different scenarios helps with establishing the fairness definition, metrics and with determining suitable follow-up steps to mitigate the bias for the disadvantaged group."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Automation Bias",
    "Sub-Threat": "N.A.",
    "Description": "When people prefer the results generated by algorithms over\nthose of humans, we speak of automation bias. It is crucial to\nremain critical when using automated systems.",
    "Vulnerability (consequence)": "",
    "Action": "The people who process or work with the results of the models must therefore be properly trained to be able to critically evaluate the generated outcomes. It is also important that the deployed models have a high degree of interpretability and understandability, so that reasoning errors can be detected more quickly in the model."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Deployment Bias",
    "Sub-Threat": "N.A.",
    "Description": "Deployment bias occurs when decision-makers and other end users behave unexpectedly with the AI system, hereby resulting in unfair <a href='https://doi.org/10.1145/3465416.3483305'>outcomes and interventions</a> .",
    "Vulnerability (consequence)": "",
    "Action": "The mitigation strategies that we mention are identical to those to mitigate automation bias: they are aimed at educating the stakeholders about the AI system and informing them about the potential risks and harms when deploying the model into the real world.\nIt�s also important to keep in mind that most issues caused by automation and deployment bias can be traced back to the abstraction and framing traps that we discussed in the first phase of the model development cycle."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Reinforcing Feedback Loop",
    "Sub-Threat": "N.A.",
    "Description": "This type of bias occurs when the output of a biased model is used to retrain the model, hereby creating a harmful feedback mechanism that amplifies historical bias.",
    "Vulnerability (consequence)": "",
    "Action": "Based on an actual case of a reinforcing feedback loop in predictive policing where arrest data was used to train the model and police officers were concentrating on already overly policed communities, researchers found that incorporating community-driven data was a suitable mitigation strategy to reduce the feedback effects. The community-driven data consisted of residents who reported on crimes. Adding data from other sources seems therefore as a suitable solution to prevent the amplification of certain undesired patterns in the model and data.\n- Since the bias is likely to be caused by measurement errors, we recommend considering the mitigation strategies discussed at Measurement Bias.\n- Add also the �neutral� labels to the new dataset. That is, in fraud detection models, add also the people to the dataset who were checked for fraud and who did not commit fraud."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Lack of explanatory capabilities",
    "Sub-Threat": "N.A.",
    "Description": "if the results of the model cannot easily be understood or interpreted, it is difficult to assess its fairness, hereby making a system vulnerable for biases. <a href='https://doi.org/10.1007/978-3-030-77211-6_1'>More details</a>",
    "Vulnerability (consequence)": "",
    "Action": "Visualisations and User Interfaces: First, to educate stakeholders about the functioning of the AI systems, create intuitive visualisations and user interfaces that give insight into how the model arrived at its outcome, including:\n- Reporting and visualising which features and values played an important role for generated decision, for example with <a href='https://proceedings.neurips.cc/paper/2017/'>SHAP</a> or <a href='https://christophm.github.io/interpretable-ml-book/pdp.html'>Partial Dependence Plots</a>\n- With how much certainty the outcome was generated. This guides end users to use their own judgments more when the model generates an outcome with low certainty.\n- Additional information to support the stakeholders� judgments."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Lack of explanatory capabilities",
    "Sub-Threat": "N.A.",
    "Description": "if the results of the model cannot easily be understood or interpreted, it is difficult to assess its fairness, hereby making a system vulnerable for biases. <a href='https://doi.org/10.1007/978-3-030-77211-6_1'>More details</a>",
    "Vulnerability (consequence)": "",
    "Action": "Model Cards: Despite all our actions in the Fairness Pipeline, each model will continue to have vulnerabilities that could potentially lead to discriminatory behavior against minority groups. The eradication of  biases is a continuous process in which human biases are interwoven into the data and the model development process in complicated ways.\nIt is therefore crucial to be transparent about these vulnerabilities, so that policy makers can take these risks into account when they write policies about the use of the model. Model cards are useful tools to document the most important information of the data, the model and its performance comprehensively.\nModel cards are short documents (one or two pages) that report the model�s performance for the demographic groups in the dataset and summarize the ethical, inclusive and fair considerations of the <a href='https://doi.org/10.1145/3287560.3287596'>model</a>. See also the Model Card Prompts in the Appendix for an overview of the components that can be included on model cards."
  },
  {
    "Category": "Fairness",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Lack of explanatory capabilities",
    "Sub-Threat": "N.A.",
    "Description": "if the results of the model cannot easily be understood or interpreted, it is difficult to assess its fairness, hereby making a system vulnerable for biases. <a href='https://doi.org/10.1007/978-3-030-77211-6_1'>More details</a>",
    "Vulnerability (consequence)": "",
    "Action": "Monitoring Plan: Finally, we want ensure that the model�s functioning can be monitored, and feedback can be provided to prevent the bias caused by a reinforcement feedback loop. This can be done with a monitoring plan describing the responsibilities and tasks of employees in monitoring the AI system after deployment. These responsibilities include:\n- Handling complaints of people on the AI system. Everyone should be able to report discriminatory or biased practices, and the appointed employees should investigate these cases;\n- Registering the errors of the model."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "(Specific ML) Ensure that models respect differential privacy (to a sufficient degree)\nDifferential privacy (DP) is a strong, mathematical definition of privacy in the context of statistical and ML analysis. According to this mathematical definition, DP is a criterion of privacy protection, which many tools for analysing sensitive personal information have been devised to satisfy. It is noticeable that this security control can greatly reduce the performance of the model. It is therefore important to estimate the need for data or model protection. Example: Differential privacy makes it possible for technology companies to collect and share aggregate information about user habits, while maintaining the privacy of individual users.\n\nhttps://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/differential-privacy/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "To implement diffierential privacy, is possible to use one of these tools:\nhttps://aip360.res.ibm.com/tools#differential\nhttps://opacus.ai/#quickstart"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "https://blog.tensorflow.org/2020/06/introducing-new-privacy-testing-library.html"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too.\nAn useful tool to keep updating on new attacks and defenses is <a href='https://atlas.mitre.org/'>this</a>."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk of data breach becomes very low. Anyway, this is a very difficult measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Testing",
    "Data type": "Privacy",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Model Inversion Attack & Membership Inference Attack",
    "Description": "A type of attack in which the attacker explores a model by providing a series of carefully crafted inputs and observing outputs. These attacks can be previous steps to more harmful types, evasion or poisoning for example. It is as if the attacker made the model talk to then better compromise it or to obtain information about it (e.g. model extraction) or its training data (e.g. membership inferences attacks and Inversion attacks). \nExample: an attacker studies the set of input-output pairs and uses the results to retrieve training data.",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Membership Inference Attack",
    "Description": "The attacker can determine whether a given data record was part of the model�s training dataset or not [1]. Researchers were able to predict a patient�s main procedure (e.g: Surgery the patient went through) based on the attributes (e.g: age, gender, hospital) [1].",
    "Vulnerability (consequence)": "The attacker can determine whether a given data record was part of the model�s training dataset or not",
    "Action": "Research papers demonstrating the viability of this attack indicate Differential Privacy [4, 9] would be an effective mitigation. This is still a nascent field at Microsoft and AETHER Security Engineering recommends building expertise with research investments in this space. This research would need to enumerate Differential Privacy capabilities and evaluate their practical effectiveness as mitigations, then design ways for these defenses to be inherited transparently on our online services platforms, similar to how compiling code in Visual Studio gives you on-by-default security protections which are transparent to the developer and users."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Oracle",
    "Sub-Threat": "Membership Inference Attack",
    "Description": "The attacker can determine whether a given data record was part of the model�s training dataset or not [1]. Researchers were able to predict a patient�s main procedure (e.g: Surgery the patient went through) based on the attributes (e.g: age, gender, hospital) [1].",
    "Vulnerability (consequence)": "The attacker can determine whether a given data record was part of the model�s training dataset or not",
    "Action": "To implement diffierential privacy, is possible to use one of these tools:\nhttps://aip360.res.ibm.com/tools#differential\nhttps://opacus.ai/#quickstart"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "RAI intervention documentation",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable RAI prevention measures where put in place",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-user-story/'>(Pattern) RAI user story</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "RAI intervention documentation",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable RAI prevention measures where put in place",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-assessment-for-test-cases/'>(Pattern) Ethical assessment for test cases</a>\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-acceptance-testing/'>(Pattern) RAI acceptance testing</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "RAI intervention documentation",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable RAI prevention measures where put in place",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-assessment-for-test-cases/'>(Pattern) Ethical assessment for test cases</a>\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/rai-acceptance-testing/'>(Pattern) RAI acceptance testing</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "RAI intervention documentation",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable RAI prevention measures where put in place",
    "Action": "<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/continuous-rai-validator/'>(Pattern) Continuous ethical validator</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 1.1 - Conduct a data protection impact assessment (DPIA) to identify risks and implement appropriate technical and organisational measures to reduce them.\nhttps://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/what-are-the-accountability-and-governance-implications-of-ai/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "To conduct DPIA with an online tool, it is possible to use <a href='https://www.enisa.europa.eu/risk-level-tool/risk'>this tool</a>."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "https://aip360.res.ibm.com/tools#assessment"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 1.7 - Document the data you will collect to train the AI system and assess whether it is accurate, adequate, relevant, and limited to your purpose(s) to demonstrate compliance with the data minimisation principle.\nhttps://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/how-should-we-assess-security-and-data-minimisation-in-ai/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 2.1 - Failing to choose an appropriate lawful basis causes the unlawful collection of personal data. As a consequence, individuals lose trust over how their data is used and suffer from unfair processing.\n\nICO 2.8 - Apply de-identification techniques to training data before it is extracted from its source and shared internally or externally.\n\nICO 3.8 - Reassess and document what data is necessary, adequate, and relevant for training and testing your AI system. Erase any data that is not needed.\n\nICO 4.4 - Document and define mechanisms to monitor the performance of your model. Where model drift is identified, assess, and delete (or anonymise) training data that is inadequate or irrelevant to your model�s performance."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "Use one of the following tools:\nhttps://aip360.res.ibm.com/tools#anonymization\nhttps://aip360.res.ibm.com/tools#minimization\nhttps://github.com/Microsoft/presidio"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 4.4 - Document and define mechanisms to monitor the performance of your model. Where model drift is identified, assess, and delete (or anonymise) training data that is inadequate or irrelevant to your model�s performance."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "RE",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 1.3 - Document each purpose for using personal data at each stage of the AI lifecycle, assess whether they are compatible with the originally defined purpose, and schedule reviews to reassess your purposes and whether they remain compatible to define what your AI system will be used for, how personal data will be used and prevent incompatible processing taking place.\nhttps://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/principles/purpose-limitation/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 3.1 - Assess and document whether your current purposes are different from your initial purposes to\n- detect any changes in the purposes of how you will deploy your AI system\n- correct any identified function creep risks. \n\nICO 2.7 - Assess, document, and publish privacy information about what personal data you will be processing and for what purposes to prevent data subjects losing control over how their personal data is processed."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "General",
    "Sub-Threat": "Unlawful consent and data acquisition",
    "Description": "Measures which not address a technical issue but are needed to make the model compliant with data regulation",
    "Vulnerability (consequence)": "Unability to demonstrate suitable data protection measures where put in place",
    "Action": "ICO 4.4 - Document and define mechanisms to monitor the performance of your model. Where model drift is identified, assess, and delete (or anonymise) training data that is inadequate or irrelevant to your model�s performance."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: Define access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Poor access rights management",
    "Action": "(Organizational) Apply a RBAC model, respecting the least privileged principle: \nDefine access rights management using a RBAC (Role Based Access Control) model respecting the least privileged principle. This should cover all components of the ML model (e.g. host infrastructures) and allow for the protection of resources such as the model (e.g. its configuration, its code) and the data it used (e.g. training data). It is notable that the roles to be included also concern the end user. For example: the end user who can submit inputs to the model should not be able to have access to its configuration."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Existence of unidentified compromise scenarios",
    "Action": "(Technical) Conduct a risk analysis of the ML application\nA risk analysis of the overall application should be conducted to take into account the specificities of its context, including: \n- The attacker�s motivations \n- The sensitivity of the data handled (e.g. medical or personal and thus subject to regulatory constraints, strategic for the company and should thus be highly protected) \n- The application hosting (e.g. through third parties services, cloud or on premise environments) \n- The model architecture (e.g. its exposition, learning methods) \n- The ML application lifecycle (e.g., model sharing"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Existence of unidentified compromise scenarios",
    "Action": "https://aip360.res.ibm.com/tools#assessment"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Existence of unidentified compromise scenarios",
    "Action": "https://aip360.res.ibm.com/tools#assessment"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Testing",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Weak access protection mechanisms for ML model components",
    "Action": "(Organizational) Ensure ML applications comply with identity management, authentication, and access control policies\nAs all applications, those using ML must comply with defined policies regarding identity management (e.g. ensure all users are integrated in the departure process), authentication (e.g. passwords complexity, use of Multi-Factors Authentication (MFA), access restriction) and access control (e.g. RBAC model, connection context). Underlying security requirements must be applied to all ML application components (e.g. model configuration, host infrastructures, training data)."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Monitoring",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "(Specific ML) Implement processes to maintain security levels of ML components over time\nML is a rapidly evolving field, especially regarding its cybersecurity. Regular checking of new attacks and defenses must be integrated into the processes for maintaining security level applications. The security level should thus be regularly assessed too."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "By keeping the data encrypted during the entire model lifecycle, the risk of data breach becomes very low. Anyway, this is a very difficult measure to apply at the moment.\n\n<a href='https://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/homomorphic-encryption/'>Encrypted-data-based trainer (pattern)</a>"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "N.A.",
    "Description": "This threat refers to the possibility of leakage of all or partial information about the model.\nExample: the outputs of a ML algorithm are so verbose that they give information about its configuration (or leakage of sensitive data)",
    "Vulnerability (consequence)": "Lack of security process to maintain a good security level of the components of the ML application",
    "Action": "To implement encryption in AI, you can use one of the following tools:\nhttps://azure.microsoft.com/en-us/blog/azure-confidential-computing/\nhttps://www.intel.com/content/www/us/en/developer/tools/homomorphic-encryption/overview.html#gs.iqs52j"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "(Specific ML) Ensure that models respect differential privacy (to a sufficient degree)\nDifferential privacy (DP) is a strong, mathematical definition of privacy in the context of statistical and ML analysis. According to this mathematical definition, DP is a criterion of privacy protection, which many tools for analysing sensitive personal information have been devised to satisfy. It is noticeable that this security control can greatly reduce the performance of the model. It is therefore important to estimate the need for data or model protection. Example: Differential privacy makes it possible for technology companies to collect and share aggregate information about user habits, while maintaining the privacy of individual users.\n\nhttps://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/differential-privacy/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "To implement diffierential privacy, is possible to use one of these tools:\nhttps://aip360.res.ibm.com/tools#differential\nhttps://opacus.ai/#quickstart"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Disclosure of sensitive data for ML algorithm training",
    "Action": "(Specific ML) Use federated learning to minimize risk of data breaches\nFederated learning is a set of training techniques that trains a model on several decentraliser servers containing local data samples, without exchanging their data samples. This avoids the need to transfer the data and/or entrust it to an untrusted third party and thus helps to preserve the privacy of the data.\n\nhttps://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/secure-multi-party-computation/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Data disclosure",
    "Description": "This threat refers to a leak of data manipulated by ML algorithms. This data leakage can be explained by an inadequate access control, a handling error of the project team or simply because sometimes the entity that owns the model and the entity that owns the data are distinct. To train the model, it is often necessary for the data to be accessed by the model provider. This involve sharing the data and thus share sensitive data with a third party.",
    "Vulnerability (consequence)": "Disclosure of sensitive data for ML algorithm training",
    "Action": "To implement federated learning in AI, you can use one of the following tools:\nhttps://www.tensorflow.org/federated\nhttps://aip360.res.ibm.com/tools#federated"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Model disclosure",
    "Description": "This threat refers to a leak of the internals (i.e. parameter values) of the ML model. This model leakage could occur because of human error or contraction with a third party with a too low security level.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "(Specific ML) Ensure that models respect differential privacy (to a sufficient degree)\nDifferential privacy (DP) is a strong, mathematical definition of privacy in the context of statistical and ML analysis. According to this mathematical definition, DP is a criterion of privacy protection, which many tools for analysing sensitive personal information have been devised to satisfy. It is noticeable that this security control can greatly reduce the performance of the model. It is therefore important to estimate the need for data or model protection. Example: Differential privacy makes it possible for technology companies to collect and share aggregate information about user habits, while maintaining the privacy of individual users.\n\nhttps://research.csiro.au/ss/science/projects/responsible-ai-pattern-catalogue/differential-privacy/"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Model disclosure",
    "Description": "This threat refers to a leak of the internals (i.e. parameter values) of the ML model. This model leakage could occur because of human error or contraction with a third party with a too low security level.",
    "Vulnerability (consequence)": "The model allows private information to be retrieved",
    "Action": "To implement diffierential privacy, is possible to use one of these tools:\nhttps://aip360.res.ibm.com/tools#differential\nhttps://opacus.ai/#quickstart"
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Design",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Model disclosure",
    "Description": "This threat refers to a leak of the internals (i.e. parameter values) of the ML model. This model leakage could occur because of human error or contraction with a third party with a too low security level.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Development",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Model disclosure",
    "Description": "This threat refers to a leak of the internals (i.e. parameter values) of the ML model. This model leakage could occur because of human error or contraction with a third party with a too low security level.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  },
  {
    "Category": "Privacy",
    "SDLC Phase": "Deployment",
    "Data type": "",
    "Local/Global Explanation": "",
    "Explanation Goal": "",
    "Threat": "Model or data disclosure",
    "Sub-Threat": "Model disclosure",
    "Description": "This threat refers to a leak of the internals (i.e. parameter values) of the ML model. This model leakage could occur because of human error or contraction with a third party with a too low security level.",
    "Vulnerability (consequence)": "Too much information about the model given in its outputs",
    "Action": "(Specific ML) Reduce the information given by the model\nControlling the information (like its verbosity) provided by the model by applying basic cybersecurity hygiene rules is a way of limiting the techniques that an attacker can use to build adversarial examples. One of the basic rules of hygiene, for example, is to reduce the information of the output determined by the model to the maximum, or by profiling who is making the request. \nFor example: considering a classification application, it would consist of communicating only the predicted class to the users of solution, not the associated probability. \nTo summarize, some of the possible valid techniques are:\n- Rate-limit queries allowed by model\n- Minimize or obfuscate the details returned in prediction APIs while still maintaining their usefulness to �honest� applications [7].\n- Define a well-formed query for your model inputs and only return results in response to completed, well-formed inputs matching that format.\n- Return rounded confidence values. Most legitimate callers do not need multiple decimal places of precision."
  }
]
