import FadeIn from "react-fade-in/lib/FadeIn";

function Learning() {
    return (
      <>

<head>
<meta charSet="utf-8" />
<meta
  name="viewport"
  content="width=device-width, initial-scale=1.0, shrink-to-fit=no"
/>
<title>Learning</title>
<link rel="stylesheet" type="text/css" href="style.css" />
<style>
  {`
    body, html {
      margin: 0;
      padding: 0;
      height: 100%;
    }

    .wrapper {
      height: 100%;
      background: #aca7eb;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .wrapper2 {
      height: 100%;
      background: #eaccf5
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .container2 {
      width: 100%; /* Imposta la larghezza al 100% */
      padding: 0; /* Elimina il padding */
      margin: 0; /* Elimina il margin */
      background: linear-gradient(to center, #eaccf5 35%, #aca7eb 77%);
      font-family: 'Lucida Fax', sans-serif; /* Imposta il font del testo su Lucida Fax */
    }
  `}
</style>
</head>


<FadeIn>
  <div className="wrapper">
    <div className="container2" style={{ 
      background: "#bebaf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px" // Aggiunge un po' di spazio intorno al contenuto
    }}>

      <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '24px' }}>
        HOW TO MAKE AI TRUSTWORTHY
        <br />
        <br />
        <br />
      </strong>

      <p>
        In order to make AI trustworthy, it is good to follow some  <strong>Guidelines. </strong>
        Trustworthy AI is based on <strong>four pillars</strong> that must be present throughout the
        system:
        <br />
        <br />

        <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> • <strong>Explainability</strong>, which ensures that each AI-based system is clear and transparent about the actions and decisions they take;</li>
        <li> • <strong>Fairness</strong>, which ensures every form of equality and respect for human rights, avoiding forms of racism or preferentialism;</li>
        <li> • <strong>Privacy</strong>, which guarantees the confidentiality of the sensitive data of each user of a system, according to the legislative regulations of the GDPR;</li>
        <li> • <strong>Security</strong>, which protects data from any type of modification or falsification attack by unauthorized persons or malicious attackers.</li>
      </ul>

        <br />
        <br />
        Each pillar, while essential, is not sufficient on its own to ensure trusted
        AI. Ideally, <strong>four three elements should interact in harmony and overlap</strong>;
        should practical tensions emerge between them, society should work to
        resolve them.
        <br />
        <br />
        <br />
        <br />
      </p>
    </div>
  </div>
</FadeIn>


  <FadeIn>
  <div className="wrapper2">
    <div className="container2" style={{ 
      background: "#eaccf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px" // Aggiunge un po' di spazio intorno al contenuto
    }}>
        

  <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '24px' }}>
  SOME DETAILS
  <br />
  <br />
  <br />
</strong>

  <p>    AI ethics constitutes a field of applied ethics dedicated to examining the
    ethical dilemmas arising from the <strong>design, deployment, and use of Artificial
    Intelligence.</strong> His main interest is understanding how AI can positively or
    negatively influence the happiness of individuals, both in terms of quality
    of life and freedom and autonomy fundamental to a democratic society.
    <br />
    <br />
    Thinking about the ethics of AI can be beneficial for several reasons. First
    of all, it can encourage <strong>the protection of individuals and groups at a basic
    level</strong>. Secondly, it can <strong>foster ethical value-driven innovation</strong>, such as
    those contributing to the achievement of the UN Sustainable Development
    Goals, which form an integral part of the EU's 2030 Agenda. Although this
    document focuses primarily on the first objective mentioned, the importance
    that ethics can have for the second should not be underestimated.
    <br />
    <br />
    Trustworthy AI can improve <strong>individual and collective well-being</strong>, generating
    wealth, creating value and maximizing well-being. It can contribute to
    building an equitable society, promoting the health and well-being of
    citizens in ways that promote equality in economic, social and political
    opportunity.
    <br />
    <br />
    It is therefore crucial to understand how to support the development,
    deployment and use of AI in the best possible way, in order <strong>to ensure that
    everyone can thrive in a world where AI is widespread and to build a better
    future while maintaining competitiveness global</strong>. The use of AI systems in
    our society, as with any advanced technology, presents several ethical
    issues, including their impact on people and society, on decisions made and
    on safety. If we rely more and more on AI systems or delegate decisions to
    them, it is essential to ensure, through adequate accountability processes,
    that <strong>the effects of such systems on people's lives are fair, respect
    fundamental values ​​and act accordingly.
    <br /></strong>
    <br />
    Although an industry-specific code of ethics, no matter how well developed
    and updated, can never be a complete replacement for the ethical reasoning
    process itself. The latter must always remain attentive to contextual
    details that cannot be fully included in general guidelines. In addition to
    developing a set of standards, <strong>it is crucial to promote and maintain an
    ethical culture and mindset through public involvement, education and
    hands-on learning.</strong>
    <br />
    <br />
    We have faith in an approach to AI ethics that is based on the fundamental
    rights established by the treaties of the European Union, the Charter of
    Fundamental Rights of the European Union and international human rights law.
    Respect for these rights, in the context of democracy and the rule of law,
    constitutes the strongest basis for identifying abstract ethical principles
    and values ​​that can be applied in AI systems.
    <br />
    <br />
    These fundamental rights find a common denominator in respect for human
    dignity, thus embodying what we define as an <strong>"anthropocentric approach"</strong>. In
    this approach, the human being holds a unique and inalienable moral status,
    being at the center of civil, political, economic and social issues.
    <br />
    <br />
    <br />
    The Fundamental rights as a basis for Trustworthy AI are the following ones:
    <br />
    <br />
    <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
    <li> • Respect for human dignity;</li>
    <li> • Freedom of the individual;</li>
    <li> • Respect for democracy, justice and the rule of law;</li>
    <li> • Equality, non-discrimination and solidarity;</li>
    <li> • Citizens' rights.</li>
    </ul>

    <br />
    <br />
    Between these principles there might be <strong>tensions </strong>and there is no default
    solution to resolve them. Addressing such tensions requires adopting methods
    of <strong>responsible discussion</strong>, consistent with the European Union's fundamental
    commitment to democratic participation, <strong>fair</strong> trial and <strong>open political
    participation.</strong>
    <br />
    <br />
    For example, in various fields of application, the principle of harm
    prevention may conflict with the principle of human autonomy. Consider the
    example of using AI systems in “predictive policing,” which could help
    reduce crime but could also compromise individual freedom and privacy due to
    surveillance. However, the overall benefits of AI systems are expected to
    significantly outweigh the foreseeable individual risks.
    <br />
    <br />
    Although these principles point the direction towards possible solutions,
    they remain abstract ethical concepts. It cannot therefore be assumed that
    AI operators will find the right solution based on these principles alone;
    however, they must address ethical dilemmas and trade-offs through <strong>rational
    reflection based on evidence rather than intuition or random decisions.</strong>
    <br />
    <br />
    In some situations, it may not be possible to identify ethically acceptable
    compromises. Some fundamental rights and related principles are absolute and
    intransigent, such as human dignity.
    <br />
    <a>
    <div id="explainability"> 

</div>
    </a> 
    <br />
    <br />
  </p>


  </div>
  </div>
  </FadeIn>

  <FadeIn>
  <div className="wrapper">
    <div className="container2" style={{ 
      background: "#bebaf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px" // Aggiunge un po' di spazio intorno al contenuto
    }}>



  <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '24px' }}>
  THE PILLAR PRINCIPLES
  <br />
  <br />
  <br />
</strong>



<strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '21px' }}>
  
  EXPLAINABILITY
  <br />
  <br />
</strong>

<div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '13%', maxWidth: '400px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/ai-explainability-svgrepo-com.svg" 
      alt="Explainability Image" 
      style={{ width: '100%', maxWidth: 'auto', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>
  
  <p> 
  <br />
  <br />
    <strong>Explainability </strong>is essential to <strong>instilling trust</strong> in users of AI systems. This
    principle implies that <strong>processes</strong> must be <strong>clear</strong>, the functionality and <strong>goals </strong>
    of AI systems must be <strong>openly communicated</strong>, and <strong>decisions</strong> must be <strong>explainable </strong>
    to the extent possible. Without such information, a decision cannot be
    adequately challenged. However, there are cases where it is not possible to
    fully explain why a model produced a specific outcome or decision, and this
    is known as the "black box" problem. In such situations, further actions may
    be required to ensure clarity (such as the ability to track, verify and
    openly communicate system capabilities), while still ensuring respect for
    fundamental rights. <strong>The amount of transparency</strong> needed depends mainly on the
    <strong> context</strong> and the <strong>severity of the consequences</strong> if the result is wrong or
    imprecise.
    <br />
    <br />
    This requirement involves the transparency of the elements relevant to an
    artificial intelligence system: the data, the system itself and the business
    models associated with it.
    <br />
    <br />
    Explainability concerns the <strong>ability to describe both the technical processes
    of an AI system and the human decisions related to it</strong>, such as application
    areas. To ensure that an AI system is technically explicable, humans must be
    able to understand and monitor the decisions made by the system. You may
    need to find a balance between improving the explainability of a system
    (even at the cost of accuracy) and increasing accuracy (at the expense of
    explainability).
    <br />
    <br />
    When an AI system has a significant impact on people's lives, it should
    always be possible to request an <strong>adequate explanation</strong> of the system's
    decision-making process. This explanation should be timely and tailored to
    the expertise of the stakeholder involved, who could be a non-expert, a
    regulator or a researcher. Guidance should be provided on how an AI system
    influences and shapes organizational decision-making, on the design choices
    of the system and on the logic behind its deployment, thus ensuring the
    transparency of the business model.
    <br />
    <br />
    <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Examples of Explainability: 
          </strong>
        <br />
        <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> • If an individual applies for a bank loan and is denied by the
    algorithm, it is their right to know why;</li>
        <li> • If an individual is excluded from a public housing selection program
    based on a prioritization algorithm, he or she has the right to understand
    the reasons for such exclusion and to have access to the information used to
    determine his or her eligibility;</li>
        <li> • If a candidate is not selected for a job through an automated resume
    screening process, he or she has the right to request a detailed explanation
    of the reasons for the rejection and to have access to the metrics used to
    evaluate his or her suitability.</li>
      </ul>
    <br />
    <br />
    <br />
  </p>
  <a>
    <div id="fairness"> 

</div>
    </a>   


</div>
</div>
</FadeIn>

<FadeIn>
<div className="wrapper2">
    <div className="container2" style={{ 
      background: "#eaccf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px" // Aggiunge un po' di spazio intorno al contenuto
    }}>
  <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '21px' }}>
  <br />
    <br />
    <br />
  FAIRNESS
  <br />
  <br />
</strong>

<div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '12%', maxWidth: '400px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/fairness-svgrepo-com.svg" 
      alt="Fairness Image" 
      style={{ width: '100%', maxWidth: '100%', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>


  <p> 
  <br />
    To ensure the reliability of an AI system, it is essential to <strong>promote
    inclusion and diversity</strong> throughout its lifecycle. This involves not only
    involving all AI-influenced stakeholders in the process, but also ensuring
    equality of treatment and access through inclusive design, which is closely
    linked to the concept of equity and <strong>Fairness</strong>.
    <br />
    <br />
    <strong>Data </strong>used to train and operate AI systems <strong>can be affected by historical
    bias, incompleteness, and mismanagement</strong>c. If these distortions persist, <strong>they
    can lead to direct and indirect prejudice and discrimination</strong>, worsening the
    marginalization of certain groups or individuals. Furthermore, they can be
    <strong> exploited to practice unfair competition</strong>, such as the homogenization of
    prices through collusion or lack of transparency in the market. Identifiable
    and discriminatory biases should be removed, if possible, already at the
    data collection stage. The development of AI systems, such as the
    programming of algorithms, can also be affected by such biases. <strong>One solution
    could be to implement surveillance processes</strong> to transparently analyze and
    address system objectives, constraints, requirements and decisions.
    Furthermore, hiring staff from diverse backgrounds can ensure a variety of
    opinions and should be encouraged.
    <br />
    <br />

    <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '20%', maxWidth: '400px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/Protected_Attributes.png" 
      alt="Attributes Image" 
      style={{ width: '100%', maxWidth: '100%', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>
<br />
<br />
    <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Examples of Fairness: 
          </strong>
        <br />
        <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> • A system that must choose between a certain number of candidates to hire must regardless of skin color;</li>
        <li> • A system for allocating medical resources in emergency situations should be based solely on the severity of the patient's condition and medical need, without taking into account factors such as social status or income;</li>
        <li> • A scholarship distribution system should evaluate students' academic merit and financial need fairly, without discriminating based on religion, political orientation or other personal characteristics.</li>
      </ul>
      <br />
      <br />
      <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          BIAS ANALYSIS 
          </strong>
        <br />
        <br />
        <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Steps 
          </strong>
          <br />
          <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> 1. Selecting the scale of the bias analysis by choosing between individual and group-level fairness</li>
        <li> 1b. Collect required information</li>
        <li> 2. Choosing a fairness definition</li>
        <li> 3. Selecting a single or multiple fairness metrics that correspond with the adopted fairness definition</li>
        <li> 4. Defining the demographic groups of interest</li>
        <li> 5. Applying the fairness metrics on the groups to compare them and find the groups for which the model produces skewed and unfavourable results</li>
        <li> 6. Determining the bias decision thresholds that determine which actions policy makers should take based on the bias analysis results</li>
      </ul>
      <br />
      <br />
      <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Concepts 
          </strong>
          <br />
          <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li>  1. Individual versus Group Fairness</li>
        <li>  1b. Requirements for the Bias Analysis</li>
      </ul>
      <br />
      <br />
      <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />


          Description 
          </strong>
          <br />
          <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
  <li><strong>1. Individual fairness</strong> examines if a model gives <strong>similar results</strong> for similar individuals, a challenging task due to difficulties in measuring individual similarity. Group fairness assesses if the model treats different demographic groups defined by sensitive characteristics differently. Focusing on group fairness is recommended for its broader range of applicable metrics.</li>
  <br />
  <li><strong>1b.</strong> Components for group-level fairness metrics include the model's results displayed in a <strong>confusion matrix</strong>, which allows for <strong>calculating performance ratios for protected groups</strong>, and identifying dataset groups based on sensitive attributes for comparison.</li>
  <br />
  <li><strong>2.</strong> A key challenge in bias analysis is <strong>defining fairness,</strong> as perceptions vary <strong>among stakeholders</strong>. Recommendations include stakeholder meetings to discuss fairness perspectives, simulations using different confusion matrix outcomes to visualize disparities, and diving into the selection of fairness metrics which helps define the fairness concept.</li>
  <br />
  <li><strong>3.</strong> <strong>Fairness metrics help compare model performance across groups</strong>. The "Aequitas Fairness tree" is a useful guide for selecting appropriate metrics and definitions, providing a structured approach for policymakers and data scientists.</li>
  <br />
  <a>
    <div id="matrix-containero"> 

</div>
    </a>
  <li><strong>4.</strong> After selecting fairness metrics,<strong> one must compute performance disparities across groups.</strong> Creating groups for comparison can be complex due to intersectional bias, with advice to consult domain experts and to create groups informed by feature and error distributions.</li>
  <br />

  <li><strong>5.</strong> Post-application of fairness metrics, <strong>stakeholders must</strong> collaboratively <strong>establish metric thresholds</strong> to conclude bias presence and magnitude, and <strong>decide on mitigating actions</strong>. Currently, there is no widely accepted method for setting these thresholds, necessitating a customized approach.</li>

</ul>

    <br />
    
    <br />
    
 
    <div  alt="Confusion_Matrix" style ={{margin: '50px 0 0 0'}}  > <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Confusion Matrix 
          </strong>
          <br /></div>

         
          
  <div  style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}>
    <div style={{ width: '1300px', maxWidth: '1300px', margin: '0' }}>


      <img 
        src="/assets/img/Confusion_Matrix.png" 
        alt="Matrix Image" 
        style={{ width: '1300px', maxWidth: '1300px', display: 'block' }} // Aggiunta maxWidth: '100%'
      />
    </div>
    <a>
    <div id="tree"> 

</div>
    </a>

  </div>



<strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
        <br />
          Decision Tree 
          <br />
          <br />
          Concepts
          <br />
          <br />
          </strong>
         

<ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
  <li>1. Disparate Representation versus Disparate Errors</li>
  <li>3. Trusting the Labels</li> 
  <li>5. Punitive versus Assistive Interventions</li>
  <li>7. Models with Punitive Interventions</li>
  <br />
</ul>

          <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '1300px', maxWidth: '1300px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/Tree_Concepts.png" 
      alt="Tree Image" 
      style={{ width: '1300px', maxWidth: '1300px', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>

<ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
  <li>
  <br />
    <strong>1- </strong> The first decision concerns choosing between metrics to evaluate whether the model is fair based on disparate representation or on disparate errors.
  <br />
  <br />
<strong>Representation-based metrics </strong>compare whether persons from both advantaged and disadvantaged groups have <strong>equal probability </strong>of being selected by the model, also named the
selection rate. This method is often used to evaluate whether persons from different groups have <strong>equal access </strong>to be selected by the model for a desired service or good, such as a loan, insurance or admission to a school programme. These metrics are Demographic Parity and Equal Selection Rate Parity.
<br /> <br />
<strong>Error-based metrics </strong>evaluate the <strong>difference in error rates </strong>across groups. Suppose to have a model that either denies or approves of a loan and one wants to know if there is a bias
against women, e.g., women are more often wrongly denied a loan. Using error-based fairness metrics, one can compare the False Negatives Rate (FNR) between men and women and determine whether women have a substantially higher False Negative Rate than men. The error-based metrics include False Positives Rate Parity, False Negatives Rate Parity and False Omission Rate Parity.</li>
<li>  <br />  <br />
  <strong>2- </strong>Returning to the Fairness Tree,  there is a choice to be made that depends on whether one can <strong>trust the labels. </strong>
Suppose to use a fraud prediction model. The dataset contains features, such as ‘transactions’ and ‘average spend’, and a label for each instance, such as ‘high risk’ or ‘low risk’. As Machine Learning models are trained on the dataset, the <strong>labels should be trustworthy </strong>to ensure that the correct patterns are learned. Generally, of most datasets we can say that we can trust the labels. If not, then other, more creative options should be sought to work with the faulty dataset, such as Counterfactual Fairness.</li>
<li><br /> <br />
  <strong>3- </strong>The <strong>distinction between punitive and assistive interventions</strong> plays an important role in our bias analysis, as it helps with <strong>determining which type of errors </strong>(e.g., False Positives or False Negatives) are most harmful.
When a model has interventions that are <strong>assistive </strong>in nature, people might be harmed when <strong>the model fails to intervene </strong>on them when they have need (Rodolfa et al., https://textbook.coleridgeinitiative.org/chap-bias.html#dealing-with-bias). A high rate of False Negatives is therefore undesirable, as it would mean that the model wrongly withholds this intervention from people. For example, for a model that decides who should receive a governmental subsidy, we could compare the False Negatives Rate across groups to see whether there is a large discrepancy between advantaged and disadvantaged groups. 
On the other hand, with <strong>punitive models,</strong> people are harmed by <strong>the intervention,</strong> which makes the False Positives more suitable to further explore during the bias analysis. Suppose we have a model that predicts which of the defendants who committed a crime are likely to reoffend. If our model produces substantially more False Positives for people with a non-Dutch ethnicity when compared with people with a Dutch ethnicity, we can say that the model discriminates against people with a non-Dutch ethnicity. 
<br /><br />
For both the assistive and the punitive models, there are different metrics available, each of which can be used to find error rate disparities across groups. The main differences between these metrics can be attributed to the size of the intervention and the type of groups that are compared with each other.
<br /><br />
The Fairness Tree shows the following four metrics applicable for models with assistive interventions:
<ul>
  <li> • Group Size-Adjusted False Negatives (FN/GSP)</li>
  <li> • False Omission Rate (FOR)</li>
  <li> • False Negative Rate (FNR)</li>
  <li></li>
</ul>

  • Recall/True Positives Parity</li>

  <br /><br />
  <li><strong>4- </strong>Three metrics fall under the scope of the punitive interventions, these are:
  <br /><br />
  • Group-Size Adjusted False Positives (FP/GSP)
<br />
  • False Discovery Rate Parity (FDR)
<br />
 • False Positive Rate Parity
<br />
<br />
As discussed, for models with punitive interventions we often investigate the False Positives during the bias analysis to prevent that innocent people are picked out by the model for an undesired intervention.</li>
</ul> 
  </p>
  <a>
    <div id="security"> 

</div>
    </a>   


</div>
</div>
</FadeIn>

<FadeIn>
<div className="wrapper">
    <div className="container2" style={{ 
      background: "#bebaf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px" // Aggiunge un po' di spazio intorno al contenuto
    }}>

  <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '21px' }}>
  <br />
    <br />
    <br />
  SECURITY
  <br />
  <br />
</strong>


   <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '11%', maxWidth: '400px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/security-svgrepo-com.svg" 
      alt="Security Image" 
      style={{ width: '100%', maxWidth: '100%', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>


  <p>    
  <br />
  <br />
    A fundamental element to guarantee the reliability of artificial
    intelligence is its technical robustness, or<strong> Security</strong>, which is closely related to the
    principle of <strong>damage prevention</strong>. For AI systems to be robust, it is essential
    to develop them with a risk prevention approach so that they operate
    reliably as expected, minimizing unintended and unexpected harm and
    <strong> preventing unacceptable harm</strong>. This includes the ability to <strong>adapt to
    potential changes</strong> in their operating environment and to manage interactions
    with other agents (human and artificial) that may behave in contradictory
    ways.
    <br />
    <br />
    Furthermore, it is important to <strong>ensure the physical and mental integrity</strong> of
    human beings. Like all software systems, AI systems must be protected from
    vulnerabilities that expose them to attacks, such as hacking. Attacks can
    target the data (via data <strong>poisoning</strong>), the model (via <strong>information leak</strong>), or
    the underlying infrastructure, whether software or hardware. If an AI system
    is attacked, for example with deceptive inputs (<strong>adversarial attack</strong>), both
    the data and the behavior of the system can be compromised, leading to
    incorrect decisions or even physical damage.
    <br />
    <br />
    To consider AI systems safe, it is necessary to take into account possible
    unintended applications (such as dual-use applications) and potential abuse
    by malicious individuals, taking measures to prevent and mitigate them.
    <br />
    <br />
    <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Examples of Security: 
          </strong>
        <br />
        <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> • In a corporate context, modification and insertion attacks of false data must be prevented to prevent the behavior of the algorithm from being modified;</li>
        <li> • In a financial risk identification application used by banking institutions, it is essential to avoid the inclusion of false or manipulated information to ensure the correctness of the assessments and prevent financial losses or regulatory violations;</li>
        <li> • In a facial recognition system used for access control, it is essential to protect biometric data from manipulation or falsification to prevent the results of the checks from being altered.</li>
      </ul>
    <br />
    <br />
    <br />
  </p>

  <a>
    <div id="privacy"> 

</div>
    </a>    


</div>
</div>
</FadeIn>

<FadeIn>
<div className="wrapper2">
    <div className="container2" style={{ 
      background: "#eaccf5", // Imposta lo sfondo con opacità
      fontFamily: "Lucida Fax, sans-serif", // Imposta il font del testo su Lucida Fax
      textAlign: "left",
      padding: "80px", // Aggiunge un po' di spazio intorno al contenuto
    }}>



      <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '21px' }}>
      <br />
    <br />
    <br />
        PRIVACY
      </strong>


      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', width: '100%', margin: '0' }}> {/* Imposta la larghezza e l'altezza al 100% del viewport e rimuove il margine */}
  <div style={{ width: '15%', maxWidth: '400px', margin: '0' }}> {/* Imposta la larghezza massima e relativa e rimuove il margine */}
    <img 
      src="/assets/img/privacy-svgrepo-com.svg" 
      alt="Privacy Image" 
      style={{ width: '100%', maxWidth: '100%', display: 'block' }} // Aggiunta maxWidth: '100%'
    />
  </div>
</div>




      <p>
      <br />
        <strong>Privacy</strong>, closely linked to the principle of harm prevention, is a
        fundamental right that is of particular importance in the context of
        artificial intelligence. To protect confidentiality, effective data
        governance is needed which includes <strong>managing the quality and integrity of
        the data used</strong>, its relevance to the sector in which the AI ​​systems will be
        deployed, access protocols and the ability to process the data in order to
        guarantee its confidentiality.
        <br />
        <br />
        AI systems must ensure data confidentiality and protection throughout the
        system lifecycle, including data initially provided by the user and data
        generated during interaction with the system. Recorded information about
        human behavior can allow AI systems to infer not only individual
        preferences, but also sensitive information such as sexual orientation, age,
        gender, and religious or political opinions. For people to have confidence
        in the management of their personal data,<strong>it is essential to ensure that
        such data is not used in a discriminatory or unlawful way.</strong>
        <br />
        <br />
        In every organization that handles personal data, protocols must be
        implemented that regulate access to such data. Such protocols should specify
        who can access the data and under what circumstances. Only qualified and
        authorized personnel, with the expertise and need to access personal data,
        should have such access.
        <br />
        <br />
        <strong style={{ display: 'block', width: '100%', textAlign: 'center', fontSize: '17px'}}>
        <br />
          Examples of Privacy: 
          </strong>

        <br />
        <ul style={{ textAlign: 'left', listStyle: 'initial', marginLeft: '2em' }}>
        <li> • In the medical field, preserving the confidentiality of a patient's medical record is of fundamental importance according to the GDPR;</li>
        <li >• In a bank's electronic financial transaction filing system, it is essential to ensure the confidentiality of customer personal data and sensitive financial information to prevent fraud and privacy violations;</li>
        <li> • In a legal practice management system, it is critical to maintain the confidentiality of client data and sensitive case information to ensure client trust and comply with privacy laws.</li>
      </ul>
      </p>


    </div>
  </div>
</FadeIn>




  </>
  )
    }

export default Learning;